{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5e526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62311ce8",
   "metadata": {},
   "source": [
    "## 数据预处理，基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343889a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>6036</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>32603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6037</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>76006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6038</td>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>14706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>6039</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>01060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>6040</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserID Gender  Age  OccupationID Zip-code\n",
       "0          1      F    1            10    48067\n",
       "1          2      M   56            16    70072\n",
       "2          3      M   25            15    55117\n",
       "3          4      M   45             7    02460\n",
       "4          5      M   25            20    55455\n",
       "...      ...    ...  ...           ...      ...\n",
       "6035    6036      F   25            15    32603\n",
       "6036    6037      F   45             1    76006\n",
       "6037    6038      F   56             1    14706\n",
       "6038    6039      F   45             0    01060\n",
       "6039    6040      M   25             6    11106\n",
       "\n",
       "[6040 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_title = ['UserId' , 'MovieID' , 'Rating' , 'timestamps']\n",
    "ratings = pd.read_table('data/ratings.dat' , sep='::' , header=None , names=rating_title , engine='python')\n",
    "\n",
    "users_title = ['UserID' , 'Gender' , 'Age' , 'OccupationID' , 'Zip-code']\n",
    "users = pd.read_table('data/users.dat' , sep = '::' , header=None , names=users_title , engine='python')\n",
    "\n",
    "movie_title = ['MovieID' , 'Title' , 'Genres']\n",
    "movies = pd.read_table('data/movies.dat' , sep='::' , header=None , names=movie_title , engine='python')\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfcb6f4",
   "metadata": {},
   "source": [
    "## 数据处理的部分\n",
    "\n",
    "UserID、Occupation和MovieID不用变。\n",
    "\n",
    "Gender字段：需要将'F'和'M'转换成0和1。\n",
    "\n",
    "Age字段：要转成7个连续数字0~6,初始数据就是表示范围的。\n",
    "\n",
    "Genres字段：是分类字段，要转成数字。首先将Genres中的类别转成字符串到数字的字典，然后再将每个电影的Genres字段\n",
    "转成数字列表，因为有些电影是多个Genres的组合。\n",
    "\n",
    "Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉。\n",
    "\n",
    "Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用‘< PAD >’对应的数字填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f878db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据，并且对数据进行预处理\n",
    "def load_data():\n",
    "    # 读取用户数据\n",
    "    users_title = ['UserID' , 'Gender' , 'Age' , 'JobID' , 'Zip-code'] # 设置表格标题\n",
    "    users = pd.read_table(\"data/users.dat\",sep = '::',header=None,names=users_title,engine='python') # 读取数据到表格中\n",
    "    users = users.filter(regex = 'UserID|Gender|Age|JobID')  # 按照列进行过滤，只保留这 4 列\n",
    "    users_origin = users.values  # 把 users的值保存这个副本\n",
    "    \n",
    "    # 对用户性别进行处理\n",
    "    gender_map = {'F':0 , 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)  # 对每个元素F和M进行0，1的map映射\n",
    "    \n",
    "    # 对用户年龄进行处理，将分散的年龄字段映射成连续的数字\n",
    "    age_map = {val:i for i,val in enumerate(set(users['Age']))} # 之所以用 set是因为去除重复\n",
    "    users['Age'] = users['Age'].map(age_map)   # Age的value 部分是 0-6\n",
    "    \n",
    "    # 读取电影数据\n",
    "    movie_title = ['MovieID' , 'Title' , 'Genres']\n",
    "    movies = pd.read_table('data/movies.dat',sep='::',header=None,names=movie_title,engine='python')\n",
    "    movies_origin = movies.values  # 把 movies的值保存这个副本\n",
    "    # 把 Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "    # 得到标题字段中的所有单词与序号的字典\n",
    "    title_map = {val:pattern.match(val).group(1) for i,val in enumerate(set(movies['Title']))}\n",
    "    # 将每个标题中的单词序列转换呈数字序列\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "    \n",
    "    # 电影类型转为数字的set\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val) # 修改当前集合，把val包含的元素全部加入到 genres_set中且去除重复\n",
    "    \n",
    "    # 长度统一，空白部分用 '<PAD>'填充\n",
    "    genres_set.add('<PAD>')\n",
    "    # 将电影类型的 set改为 map  value部分是电影类型所对应的 int\n",
    "    genres2int = {val:i for i , val in enumerate(genres_set)}\n",
    "    \n",
    "    #最右侧循环是取得每条电影数据的电影风格字段值，中间循环是将每个电影风格字段值拆开，因为一部电影有多种风格\n",
    "    # 下面得到的是一个 map, key 是 电影类型（可以组合），value是个List，表示电影类型的 数字组合\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for i,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    \n",
    "    #将电影类型转成等长数字列表，长度是18，因为一共18种电影，对长度不够的填充<PAD>\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "    \n",
    "    # 电影标题转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():   # 没有参数，按照空格换行符制表符等分割\n",
    "        title_set.update(val)\n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:i for i , val in enumerate(title_set)}  # 生成电影标题与序号的字典\n",
    "    \n",
    "    #将电影标题 Title转为等长的数字列表，长度为15，做法跟电影类型的处理相同\n",
    "    title_count = 15\n",
    "    # key是电影Title，value 是电影Title分割出来的词的int 表示\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for i,val in enumerate(set(movies['Title']))}\n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    # 这样就把原先 movies['Title']改为了 List[int]表示\n",
    "    movies['Title'] = movies['Title'].map(title_map) \n",
    "    \n",
    "    \n",
    "    # 读取评分数据集\n",
    "    ratings_title = ['UserID' , 'MovieID' , 'ratings' , 'timestamps']\n",
    "    ratings = pd.read_table('data/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "    \n",
    "    # 合并三个表\n",
    "    data = pd.merge(pd.merge(ratings , users) , movies)\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_origin, users_origin    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4d911a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>6036</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>32603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6037</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>76006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6038</td>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>14706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>6039</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>01060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>6040</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserID Gender  Age  OccupationID Zip-code\n",
       "0          1      F    1            10    48067\n",
       "1          2      M   56            16    70072\n",
       "2          3      M   25            15    55117\n",
       "3          4      M   45             7    02460\n",
       "4          5      M   25            20    55455\n",
       "...      ...    ...  ...           ...      ...\n",
       "6035    6036      F   25            15    32603\n",
       "6036    6037      F   45             1    76006\n",
       "6037    6038      F   56             1    14706\n",
       "6038    6039      F   45             0    01060\n",
       "6039    6040      M   25             6    11106\n",
       "\n",
       "[6040 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc099ef",
   "metadata": {},
   "source": [
    "### 加载数据，并且保存到本地\n",
    "\n",
    "title_count：Title字段的长度（15）\n",
    "title_set：Title文本的集合，保存的是各个电影Title拆分后的词\n",
    "genres2int：电影类型转数字的字典，保存的也是词\n",
    "features：是输入X,是除了 ratings的其他部分的数据\n",
    "targets_values：是学习目标y，也就是 ratings的数据\n",
    "ratings：评分数据集的Pandas对象\n",
    "users：用户数据集的Pandas对象\n",
    "movies：电影数据的Pandas对象\n",
    "data：三个数据集组合在一起的Pandas对象\n",
    "movies_orig：没有做数据处理的原始电影数据\n",
    "users_orig：没有做数据处理的原始用户数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d9d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列化对象，并将结果数据流写入到文件对象中\n",
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "# 保存在本地\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('data/preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960d0905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef11f136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[4390, 1995, 3528, 3528, 3528, 3528, 3528, 352...</td>\n",
       "      <td>[1, 11, 15, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1889, 3528, 3528, 3528, 3528, 3528, 3528, 352...</td>\n",
       "      <td>[18, 11, 13, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[356, 4198, 1008, 3528, 3528, 3528, 3528, 3528...</td>\n",
       "      <td>[15, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[622, 1291, 4805, 3528, 3528, 3528, 3528, 3528...</td>\n",
       "      <td>[15, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[3522, 4295, 173, 2743, 2782, 4641, 3528, 3528...</td>\n",
       "      <td>[15, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [4390, 1995, 3528, 3528, 3528, 3528, 3528, 352...   \n",
       "1        2  [1889, 3528, 3528, 3528, 3528, 3528, 3528, 352...   \n",
       "2        3  [356, 4198, 1008, 3528, 3528, 3528, 3528, 3528...   \n",
       "3        4  [622, 1291, 4805, 3528, 3528, 3528, 3528, 3528...   \n",
       "4        5  [3522, 4295, 173, 2743, 2782, 4641, 3528, 3528...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [1, 11, 15, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9...  \n",
       "1  [18, 11, 13, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...  \n",
       "2  [15, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9...  \n",
       "3  [15, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,...  \n",
       "4  [15, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f01fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([4390, 1995, 3528, 3528, 3528, 3528, 3528, 3528, 3528, 3528, 3528, 3528, 3528, 3528, 3528]),\n",
       "       list([1, 11, 15, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb5ce2",
   "metadata": {},
   "source": [
    "## 从本地读取进行了特征工程后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e09b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从本地读取数据\n",
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('data/preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45a9e6",
   "metadata": {},
   "source": [
    "## 搭建网络\n",
    "### 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f866604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31403f61",
   "metadata": {},
   "source": [
    "## 编码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025fe96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "# 用户ID个数,take()是ndarray的方法，第1个参数的列，第二个参数是axis，表示得到列中对应的各个行组成的 ndarray\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "# 性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48863b4d",
   "metadata": {},
   "source": [
    "### 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd6312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练次数\n",
    "num_epochs = 5\n",
    "# 批量大小\n",
    "batch_size = 256\n",
    "# 丢弃率\n",
    "dropout_keep = 0.5\n",
    "# 学习率\n",
    "learning_rate = 0.0001\n",
    "# 每处理N个批量数据显示一次统计信息\n",
    "show_every_n_batches = 20\n",
    "# 训练好的模型文件存放位置\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b34a05",
   "metadata": {},
   "source": [
    "### 定义输入占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef84b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    # 用户id\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    # 性别\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    # 年龄\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    # 职业\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    # 电影id\n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    # 电影类型\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    # 电影标题\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    # 学习目标，就是评分\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    # 学习率\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    # 丢弃率\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091e69f",
   "metadata": {},
   "source": [
    "## 构建神经网络\n",
    "### 定义User的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea370839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):   # 经过这个就少了很多列了\n",
    "        # 定义用户id的嵌入矩阵，（id总数，32），填入均匀的随机数\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        # 经过嵌入层\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "        # 定义性别的嵌入矩阵，（2，16），填入均匀的随机数\n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        # 经过嵌入层\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        # 定义年龄的嵌入矩阵，（7，16），填入均匀的随机数\n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        # 经过嵌入层\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        # 定义职业的嵌入矩阵，（21，16），填入均匀的随机数\n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        # 经过嵌入层\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da490ff",
   "metadata": {},
   "source": [
    "### 将User的嵌入矩阵一起全连接生成 User 的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ff1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接，将用户id、性别、年龄和职业的嵌入向量传给全连接层，输出维度为32\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        #第二层全连接,首先将第一层全连接层的输出连接在一起，4*32=128，（1，128），再次传给全连接层，输出维度是（1，200）\n",
    "        # fully_connected默认是relu函数，dense那个默认是线性函数，但是上面我们设置成了relu函数，这两个函数都是全连接层的意思\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3fa917",
   "metadata": {},
   "source": [
    "### 定义Movie ID的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a008df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        # 定义电影id的嵌入矩阵，（3952，32），填入均匀的随机数\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        # 经过嵌入层\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709c76b",
   "metadata": {},
   "source": [
    "### 对电影类型的多个嵌入向量做加和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8091cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        # 定义电影类型的嵌入矩阵，（19，32），填入均匀的随机数\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        # 经过嵌入层\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            # axis=1就是按行求和，有几行结果就有几个元素，可以理解为数据压缩\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "#         elif combiner == \"mean\":\n",
    "#             movie_categories_embed_layer = tf.reduce_mean(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    \n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed234b34",
   "metadata": {},
   "source": [
    "### Movie Title的文本卷积网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50b46909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        # 定义电影名的嵌入矩阵，（5216，32），填入均匀的随机数\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        # 经过嵌入层\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        # 组成4维的输入，对于图片，是 图片数量, 图片高度, 图片宽度, 图像通道数\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            # 卷积层的设置，卷积核的权重和偏差\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            # 卷积层，输入4维、卷积核4维、四个维度的步长[1,1,1,1]\n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            # 对于每个元素，将大于0的数保持不变，小于0的数置为0\n",
    "            # 一般 relu层都是跟在卷积操作+偏置 的后面\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            # 最大池化\n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            # 把卷积层+relu层+池化层 一起组合添加到 list中\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    #Dropout层\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        # 回忆文本卷积网络，把各个层连接起来,组合池化后的全部特征并拉平\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "        # dropout 防止过拟合 \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a47f84",
   "metadata": {},
   "source": [
    "### 将Movie的各个层一起做全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d23f51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #电影ID 和 电影类型 的全连接层\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        # 电影ID、电影类型和 电影名进行dropout后的文本卷积层 连接，输出维度为200  \n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "        \n",
    "        #铺平 \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398fb3ba",
   "metadata": {},
   "source": [
    "## 构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85ad1356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-e84ad41a42d2>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\lisij\\Envs\\cnn\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-17-94dd60c75f5e>:9: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-18-1e8b6785e1ad>:35: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\lisij\\Envs\\cnn\\lib\\site-packages\\tensorflow_core\\python\\ops\\losses\\losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor张量\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf0e70",
   "metadata": {},
   "source": [
    "## 取得batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "773dd24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7624f",
   "metadata": {},
   "source": [
    "## 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd67cd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\lisij\\Desktop\\movie\\movie_recommend_协同过滤、矩阵分解（要）\\runs\\1644592221\n",
      "\n",
      "2022-02-11T23:10:24.314650: Epoch   0 Batch    0/3125   train_loss = 25.451\n",
      "2022-02-11T23:10:25.121925: Epoch   0 Batch   20/3125   train_loss = 5.335\n",
      "2022-02-11T23:10:25.993805: Epoch   0 Batch   40/3125   train_loss = 3.202\n",
      "2022-02-11T23:10:26.883245: Epoch   0 Batch   60/3125   train_loss = 2.445\n",
      "2022-02-11T23:10:27.672643: Epoch   0 Batch   80/3125   train_loss = 2.129\n",
      "2022-02-11T23:10:28.477741: Epoch   0 Batch  100/3125   train_loss = 2.245\n",
      "2022-02-11T23:10:29.261609: Epoch   0 Batch  120/3125   train_loss = 1.959\n",
      "2022-02-11T23:10:30.116925: Epoch   0 Batch  140/3125   train_loss = 2.215\n",
      "2022-02-11T23:10:30.926414: Epoch   0 Batch  160/3125   train_loss = 1.445\n",
      "2022-02-11T23:10:31.684694: Epoch   0 Batch  180/3125   train_loss = 1.689\n",
      "2022-02-11T23:10:32.492270: Epoch   0 Batch  200/3125   train_loss = 1.783\n",
      "2022-02-11T23:10:33.254084: Epoch   0 Batch  220/3125   train_loss = 1.521\n",
      "2022-02-11T23:10:34.069945: Epoch   0 Batch  240/3125   train_loss = 1.525\n",
      "2022-02-11T23:10:34.837383: Epoch   0 Batch  260/3125   train_loss = 1.390\n",
      "2022-02-11T23:10:35.620144: Epoch   0 Batch  280/3125   train_loss = 1.661\n",
      "2022-02-11T23:10:36.366361: Epoch   0 Batch  300/3125   train_loss = 1.444\n",
      "2022-02-11T23:10:37.155300: Epoch   0 Batch  320/3125   train_loss = 1.500\n",
      "2022-02-11T23:10:37.948773: Epoch   0 Batch  340/3125   train_loss = 1.361\n",
      "2022-02-11T23:10:38.597336: Epoch   0 Batch  360/3125   train_loss = 1.463\n",
      "2022-02-11T23:10:39.421619: Epoch   0 Batch  380/3125   train_loss = 1.400\n",
      "2022-02-11T23:10:40.197048: Epoch   0 Batch  400/3125   train_loss = 1.286\n",
      "2022-02-11T23:10:40.931574: Epoch   0 Batch  420/3125   train_loss = 1.259\n",
      "2022-02-11T23:10:41.585468: Epoch   0 Batch  440/3125   train_loss = 1.425\n",
      "2022-02-11T23:10:42.288054: Epoch   0 Batch  460/3125   train_loss = 1.422\n",
      "2022-02-11T23:10:43.003245: Epoch   0 Batch  480/3125   train_loss = 1.447\n",
      "2022-02-11T23:10:43.631767: Epoch   0 Batch  500/3125   train_loss = 0.985\n",
      "2022-02-11T23:10:44.343499: Epoch   0 Batch  520/3125   train_loss = 1.408\n",
      "2022-02-11T23:10:45.157768: Epoch   0 Batch  540/3125   train_loss = 1.319\n",
      "2022-02-11T23:10:45.933574: Epoch   0 Batch  560/3125   train_loss = 1.492\n",
      "2022-02-11T23:10:46.732216: Epoch   0 Batch  580/3125   train_loss = 1.453\n",
      "2022-02-11T23:10:47.522878: Epoch   0 Batch  600/3125   train_loss = 1.521\n",
      "2022-02-11T23:10:48.329289: Epoch   0 Batch  620/3125   train_loss = 1.457\n",
      "2022-02-11T23:10:49.136436: Epoch   0 Batch  640/3125   train_loss = 1.336\n",
      "2022-02-11T23:10:49.870385: Epoch   0 Batch  660/3125   train_loss = 1.367\n",
      "2022-02-11T23:10:50.647386: Epoch   0 Batch  680/3125   train_loss = 1.206\n",
      "2022-02-11T23:10:51.406025: Epoch   0 Batch  700/3125   train_loss = 1.248\n",
      "2022-02-11T23:10:52.100241: Epoch   0 Batch  720/3125   train_loss = 1.237\n",
      "2022-02-11T23:10:52.740388: Epoch   0 Batch  740/3125   train_loss = 1.324\n",
      "2022-02-11T23:10:53.515236: Epoch   0 Batch  760/3125   train_loss = 1.413\n",
      "2022-02-11T23:10:54.350176: Epoch   0 Batch  780/3125   train_loss = 1.458\n",
      "2022-02-11T23:10:55.103682: Epoch   0 Batch  800/3125   train_loss = 1.270\n",
      "2022-02-11T23:10:55.904427: Epoch   0 Batch  820/3125   train_loss = 1.149\n",
      "2022-02-11T23:10:56.676074: Epoch   0 Batch  840/3125   train_loss = 1.234\n",
      "2022-02-11T23:10:57.493944: Epoch   0 Batch  860/3125   train_loss = 1.179\n",
      "2022-02-11T23:10:58.325157: Epoch   0 Batch  880/3125   train_loss = 1.206\n",
      "2022-02-11T23:10:59.056633: Epoch   0 Batch  900/3125   train_loss = 1.208\n",
      "2022-02-11T23:10:59.885541: Epoch   0 Batch  920/3125   train_loss = 1.227\n",
      "2022-02-11T23:11:00.744743: Epoch   0 Batch  940/3125   train_loss = 1.396\n",
      "2022-02-11T23:11:01.423892: Epoch   0 Batch  960/3125   train_loss = 1.398\n",
      "2022-02-11T23:11:02.240482: Epoch   0 Batch  980/3125   train_loss = 1.321\n",
      "2022-02-11T23:11:02.902449: Epoch   0 Batch 1000/3125   train_loss = 1.205\n",
      "2022-02-11T23:11:03.730416: Epoch   0 Batch 1020/3125   train_loss = 1.268\n",
      "2022-02-11T23:11:04.606766: Epoch   0 Batch 1040/3125   train_loss = 1.201\n",
      "2022-02-11T23:11:05.486033: Epoch   0 Batch 1060/3125   train_loss = 1.393\n",
      "2022-02-11T23:11:06.325551: Epoch   0 Batch 1080/3125   train_loss = 1.185\n",
      "2022-02-11T23:11:07.026182: Epoch   0 Batch 1100/3125   train_loss = 1.370\n",
      "2022-02-11T23:11:07.838958: Epoch   0 Batch 1120/3125   train_loss = 1.267\n",
      "2022-02-11T23:11:08.657635: Epoch   0 Batch 1140/3125   train_loss = 1.458\n",
      "2022-02-11T23:11:09.513859: Epoch   0 Batch 1160/3125   train_loss = 1.310\n",
      "2022-02-11T23:11:10.280756: Epoch   0 Batch 1180/3125   train_loss = 1.160\n",
      "2022-02-11T23:11:11.114970: Epoch   0 Batch 1200/3125   train_loss = 1.209\n",
      "2022-02-11T23:11:11.947285: Epoch   0 Batch 1220/3125   train_loss = 1.157\n",
      "2022-02-11T23:11:12.807769: Epoch   0 Batch 1240/3125   train_loss = 1.147\n",
      "2022-02-11T23:11:13.688806: Epoch   0 Batch 1260/3125   train_loss = 1.178\n",
      "2022-02-11T23:11:14.569053: Epoch   0 Batch 1280/3125   train_loss = 1.214\n",
      "2022-02-11T23:11:15.419351: Epoch   0 Batch 1300/3125   train_loss = 1.256\n",
      "2022-02-11T23:11:16.306833: Epoch   0 Batch 1320/3125   train_loss = 1.212\n",
      "2022-02-11T23:11:17.095096: Epoch   0 Batch 1340/3125   train_loss = 1.062\n",
      "2022-02-11T23:11:17.788570: Epoch   0 Batch 1360/3125   train_loss = 1.251\n",
      "2022-02-11T23:11:18.618151: Epoch   0 Batch 1380/3125   train_loss = 1.069\n",
      "2022-02-11T23:11:19.436619: Epoch   0 Batch 1400/3125   train_loss = 1.264\n",
      "2022-02-11T23:11:20.152577: Epoch   0 Batch 1420/3125   train_loss = 1.281\n",
      "2022-02-11T23:11:20.948211: Epoch   0 Batch 1440/3125   train_loss = 1.068\n",
      "2022-02-11T23:11:21.770802: Epoch   0 Batch 1460/3125   train_loss = 1.232\n",
      "2022-02-11T23:11:22.548176: Epoch   0 Batch 1480/3125   train_loss = 1.200\n",
      "2022-02-11T23:11:23.442837: Epoch   0 Batch 1500/3125   train_loss = 1.345\n",
      "2022-02-11T23:11:24.244734: Epoch   0 Batch 1520/3125   train_loss = 1.256\n",
      "2022-02-11T23:11:25.105216: Epoch   0 Batch 1540/3125   train_loss = 1.272\n",
      "2022-02-11T23:11:25.931426: Epoch   0 Batch 1560/3125   train_loss = 1.184\n",
      "2022-02-11T23:11:26.792347: Epoch   0 Batch 1580/3125   train_loss = 1.218\n",
      "2022-02-11T23:11:27.675037: Epoch   0 Batch 1600/3125   train_loss = 1.232\n",
      "2022-02-11T23:11:28.539230: Epoch   0 Batch 1620/3125   train_loss = 1.200\n",
      "2022-02-11T23:11:29.231299: Epoch   0 Batch 1640/3125   train_loss = 1.274\n",
      "2022-02-11T23:11:29.874745: Epoch   0 Batch 1660/3125   train_loss = 1.292\n",
      "2022-02-11T23:11:30.799535: Epoch   0 Batch 1680/3125   train_loss = 1.208\n",
      "2022-02-11T23:11:31.632573: Epoch   0 Batch 1700/3125   train_loss = 0.992\n",
      "2022-02-11T23:11:32.495863: Epoch   0 Batch 1720/3125   train_loss = 1.163\n",
      "2022-02-11T23:11:33.326042: Epoch   0 Batch 1740/3125   train_loss = 1.278\n",
      "2022-02-11T23:11:34.054568: Epoch   0 Batch 1760/3125   train_loss = 1.321\n",
      "2022-02-11T23:11:34.835657: Epoch   0 Batch 1780/3125   train_loss = 1.102\n",
      "2022-02-11T23:11:35.672291: Epoch   0 Batch 1800/3125   train_loss = 1.232\n",
      "2022-02-11T23:11:36.512391: Epoch   0 Batch 1820/3125   train_loss = 1.193\n",
      "2022-02-11T23:11:37.218872: Epoch   0 Batch 1840/3125   train_loss = 1.318\n",
      "2022-02-11T23:11:37.918765: Epoch   0 Batch 1860/3125   train_loss = 1.256\n",
      "2022-02-11T23:11:38.645670: Epoch   0 Batch 1880/3125   train_loss = 1.307\n",
      "2022-02-11T23:11:39.338194: Epoch   0 Batch 1900/3125   train_loss = 0.986\n",
      "2022-02-11T23:11:40.082549: Epoch   0 Batch 1920/3125   train_loss = 1.164\n",
      "2022-02-11T23:11:40.809776: Epoch   0 Batch 1940/3125   train_loss = 1.086\n",
      "2022-02-11T23:11:41.572664: Epoch   0 Batch 1960/3125   train_loss = 1.150\n",
      "2022-02-11T23:11:42.350967: Epoch   0 Batch 1980/3125   train_loss = 1.064\n",
      "2022-02-11T23:11:43.152455: Epoch   0 Batch 2000/3125   train_loss = 1.373\n",
      "2022-02-11T23:11:44.022084: Epoch   0 Batch 2020/3125   train_loss = 1.344\n",
      "2022-02-11T23:11:44.823990: Epoch   0 Batch 2040/3125   train_loss = 1.130\n",
      "2022-02-11T23:11:45.575209: Epoch   0 Batch 2060/3125   train_loss = 1.021\n",
      "2022-02-11T23:11:46.380143: Epoch   0 Batch 2080/3125   train_loss = 1.278\n",
      "2022-02-11T23:11:47.102533: Epoch   0 Batch 2100/3125   train_loss = 1.119\n",
      "2022-02-11T23:11:47.804694: Epoch   0 Batch 2120/3125   train_loss = 1.067\n",
      "2022-02-11T23:11:48.524203: Epoch   0 Batch 2140/3125   train_loss = 1.167\n",
      "2022-02-11T23:11:49.199285: Epoch   0 Batch 2160/3125   train_loss = 1.127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-11T23:11:49.975972: Epoch   0 Batch 2180/3125   train_loss = 1.132\n",
      "2022-02-11T23:11:50.726289: Epoch   0 Batch 2200/3125   train_loss = 1.137\n",
      "2022-02-11T23:11:51.451785: Epoch   0 Batch 2220/3125   train_loss = 1.173\n",
      "2022-02-11T23:11:52.179203: Epoch   0 Batch 2240/3125   train_loss = 0.970\n",
      "2022-02-11T23:11:52.900336: Epoch   0 Batch 2260/3125   train_loss = 1.052\n",
      "2022-02-11T23:11:53.624476: Epoch   0 Batch 2280/3125   train_loss = 1.175\n",
      "2022-02-11T23:11:54.327317: Epoch   0 Batch 2300/3125   train_loss = 1.220\n",
      "2022-02-11T23:11:55.099059: Epoch   0 Batch 2320/3125   train_loss = 1.281\n",
      "2022-02-11T23:11:55.844910: Epoch   0 Batch 2340/3125   train_loss = 1.230\n",
      "2022-02-11T23:11:56.669302: Epoch   0 Batch 2360/3125   train_loss = 1.198\n",
      "2022-02-11T23:11:57.419408: Epoch   0 Batch 2380/3125   train_loss = 1.215\n",
      "2022-02-11T23:11:58.188164: Epoch   0 Batch 2400/3125   train_loss = 1.222\n",
      "2022-02-11T23:11:59.015150: Epoch   0 Batch 2420/3125   train_loss = 1.171\n",
      "2022-02-11T23:11:59.831564: Epoch   0 Batch 2440/3125   train_loss = 1.196\n",
      "2022-02-11T23:12:00.618569: Epoch   0 Batch 2460/3125   train_loss = 1.089\n",
      "2022-02-11T23:12:01.315287: Epoch   0 Batch 2480/3125   train_loss = 1.265\n",
      "2022-02-11T23:12:02.085318: Epoch   0 Batch 2500/3125   train_loss = 1.172\n",
      "2022-02-11T23:12:02.838392: Epoch   0 Batch 2520/3125   train_loss = 1.121\n",
      "2022-02-11T23:12:03.693864: Epoch   0 Batch 2540/3125   train_loss = 1.042\n",
      "2022-02-11T23:12:04.509504: Epoch   0 Batch 2560/3125   train_loss = 0.879\n",
      "2022-02-11T23:12:05.348862: Epoch   0 Batch 2580/3125   train_loss = 1.097\n",
      "2022-02-11T23:12:06.215107: Epoch   0 Batch 2600/3125   train_loss = 1.150\n",
      "2022-02-11T23:12:07.037240: Epoch   0 Batch 2620/3125   train_loss = 1.090\n",
      "2022-02-11T23:12:07.697298: Epoch   0 Batch 2640/3125   train_loss = 1.125\n",
      "2022-02-11T23:12:08.455539: Epoch   0 Batch 2660/3125   train_loss = 1.257\n",
      "2022-02-11T23:12:09.247354: Epoch   0 Batch 2680/3125   train_loss = 1.047\n",
      "2022-02-11T23:12:10.022586: Epoch   0 Batch 2700/3125   train_loss = 1.146\n",
      "2022-02-11T23:12:10.788959: Epoch   0 Batch 2720/3125   train_loss = 1.148\n",
      "2022-02-11T23:12:11.625124: Epoch   0 Batch 2740/3125   train_loss = 1.220\n",
      "2022-02-11T23:12:12.452420: Epoch   0 Batch 2760/3125   train_loss = 1.239\n",
      "2022-02-11T23:12:13.273328: Epoch   0 Batch 2780/3125   train_loss = 1.020\n",
      "2022-02-11T23:12:13.932626: Epoch   0 Batch 2800/3125   train_loss = 1.396\n",
      "2022-02-11T23:12:14.735664: Epoch   0 Batch 2820/3125   train_loss = 1.370\n",
      "2022-02-11T23:12:15.550123: Epoch   0 Batch 2840/3125   train_loss = 1.169\n",
      "2022-02-11T23:12:16.288473: Epoch   0 Batch 2860/3125   train_loss = 1.122\n",
      "2022-02-11T23:12:17.104393: Epoch   0 Batch 2880/3125   train_loss = 1.207\n",
      "2022-02-11T23:12:17.940375: Epoch   0 Batch 2900/3125   train_loss = 1.097\n",
      "2022-02-11T23:12:18.811239: Epoch   0 Batch 2920/3125   train_loss = 1.180\n",
      "2022-02-11T23:12:19.488716: Epoch   0 Batch 2940/3125   train_loss = 1.090\n",
      "2022-02-11T23:12:20.277357: Epoch   0 Batch 2960/3125   train_loss = 1.168\n",
      "2022-02-11T23:12:21.050852: Epoch   0 Batch 2980/3125   train_loss = 1.135\n",
      "2022-02-11T23:12:21.762929: Epoch   0 Batch 3000/3125   train_loss = 1.127\n",
      "2022-02-11T23:12:22.560519: Epoch   0 Batch 3020/3125   train_loss = 1.208\n",
      "2022-02-11T23:12:23.287643: Epoch   0 Batch 3040/3125   train_loss = 1.102\n",
      "2022-02-11T23:12:23.962347: Epoch   0 Batch 3060/3125   train_loss = 1.130\n",
      "2022-02-11T23:12:24.811870: Epoch   0 Batch 3080/3125   train_loss = 1.244\n",
      "2022-02-11T23:12:25.529492: Epoch   0 Batch 3100/3125   train_loss = 1.286\n",
      "2022-02-11T23:12:26.404496: Epoch   0 Batch 3120/3125   train_loss = 1.042\n",
      "2022-02-11T23:12:26.745641: Epoch   0 Batch    0/781   test_loss = 0.992\n",
      "2022-02-11T23:12:26.960862: Epoch   0 Batch   20/781   test_loss = 1.089\n",
      "2022-02-11T23:12:27.212876: Epoch   0 Batch   40/781   test_loss = 1.063\n",
      "2022-02-11T23:12:27.402394: Epoch   0 Batch   60/781   test_loss = 1.289\n",
      "2022-02-11T23:12:27.671058: Epoch   0 Batch   80/781   test_loss = 1.446\n",
      "2022-02-11T23:12:27.902257: Epoch   0 Batch  100/781   test_loss = 1.327\n",
      "2022-02-11T23:12:28.113167: Epoch   0 Batch  120/781   test_loss = 1.200\n",
      "2022-02-11T23:12:28.346802: Epoch   0 Batch  140/781   test_loss = 1.121\n",
      "2022-02-11T23:12:28.551655: Epoch   0 Batch  160/781   test_loss = 1.296\n",
      "2022-02-11T23:12:28.825886: Epoch   0 Batch  180/781   test_loss = 1.215\n",
      "2022-02-11T23:12:29.070349: Epoch   0 Batch  200/781   test_loss = 1.169\n",
      "2022-02-11T23:12:29.271762: Epoch   0 Batch  220/781   test_loss = 0.938\n",
      "2022-02-11T23:12:29.494623: Epoch   0 Batch  240/781   test_loss = 1.222\n",
      "2022-02-11T23:12:29.743774: Epoch   0 Batch  260/781   test_loss = 1.116\n",
      "2022-02-11T23:12:29.991827: Epoch   0 Batch  280/781   test_loss = 1.413\n",
      "2022-02-11T23:12:30.219303: Epoch   0 Batch  300/781   test_loss = 1.181\n",
      "2022-02-11T23:12:30.418181: Epoch   0 Batch  320/781   test_loss = 1.330\n",
      "2022-02-11T23:12:30.647281: Epoch   0 Batch  340/781   test_loss = 0.871\n",
      "2022-02-11T23:12:30.875652: Epoch   0 Batch  360/781   test_loss = 1.259\n",
      "2022-02-11T23:12:31.160965: Epoch   0 Batch  380/781   test_loss = 1.138\n",
      "2022-02-11T23:12:31.391084: Epoch   0 Batch  400/781   test_loss = 1.074\n",
      "2022-02-11T23:12:31.618136: Epoch   0 Batch  420/781   test_loss = 1.010\n",
      "2022-02-11T23:12:31.836928: Epoch   0 Batch  440/781   test_loss = 1.200\n",
      "2022-02-11T23:12:32.081080: Epoch   0 Batch  460/781   test_loss = 1.123\n",
      "2022-02-11T23:12:32.326633: Epoch   0 Batch  480/781   test_loss = 1.089\n",
      "2022-02-11T23:12:32.549945: Epoch   0 Batch  500/781   test_loss = 0.976\n",
      "2022-02-11T23:12:32.814776: Epoch   0 Batch  520/781   test_loss = 1.168\n",
      "2022-02-11T23:12:33.064158: Epoch   0 Batch  540/781   test_loss = 0.989\n",
      "2022-02-11T23:12:33.270104: Epoch   0 Batch  560/781   test_loss = 1.294\n",
      "2022-02-11T23:12:33.511301: Epoch   0 Batch  580/781   test_loss = 1.170\n",
      "2022-02-11T23:12:33.783852: Epoch   0 Batch  600/781   test_loss = 1.181\n",
      "2022-02-11T23:12:34.008719: Epoch   0 Batch  620/781   test_loss = 1.197\n",
      "2022-02-11T23:12:34.275081: Epoch   0 Batch  640/781   test_loss = 1.216\n",
      "2022-02-11T23:12:34.470309: Epoch   0 Batch  660/781   test_loss = 1.143\n",
      "2022-02-11T23:12:34.678017: Epoch   0 Batch  680/781   test_loss = 1.445\n",
      "2022-02-11T23:12:34.915236: Epoch   0 Batch  700/781   test_loss = 1.100\n",
      "2022-02-11T23:12:35.142959: Epoch   0 Batch  720/781   test_loss = 1.287\n",
      "2022-02-11T23:12:35.366050: Epoch   0 Batch  740/781   test_loss = 1.154\n",
      "2022-02-11T23:12:35.623815: Epoch   0 Batch  760/781   test_loss = 1.135\n",
      "2022-02-11T23:12:35.856459: Epoch   0 Batch  780/781   test_loss = 1.172\n",
      "2022-02-11T23:12:36.901939: Epoch   1 Batch   15/3125   train_loss = 1.186\n",
      "2022-02-11T23:12:37.729653: Epoch   1 Batch   35/3125   train_loss = 1.152\n",
      "2022-02-11T23:12:38.450912: Epoch   1 Batch   55/3125   train_loss = 1.301\n",
      "2022-02-11T23:12:39.251775: Epoch   1 Batch   75/3125   train_loss = 1.107\n",
      "2022-02-11T23:12:40.092578: Epoch   1 Batch   95/3125   train_loss = 0.965\n",
      "2022-02-11T23:12:40.907431: Epoch   1 Batch  115/3125   train_loss = 1.187\n",
      "2022-02-11T23:12:41.459159: Epoch   1 Batch  135/3125   train_loss = 0.993\n",
      "2022-02-11T23:12:42.186173: Epoch   1 Batch  155/3125   train_loss = 1.113\n",
      "2022-02-11T23:12:42.989462: Epoch   1 Batch  175/3125   train_loss = 1.087\n",
      "2022-02-11T23:12:43.691581: Epoch   1 Batch  195/3125   train_loss = 1.219\n",
      "2022-02-11T23:12:44.506177: Epoch   1 Batch  215/3125   train_loss = 1.123\n",
      "2022-02-11T23:12:45.232927: Epoch   1 Batch  235/3125   train_loss = 1.104\n",
      "2022-02-11T23:12:46.023816: Epoch   1 Batch  255/3125   train_loss = 1.225\n",
      "2022-02-11T23:12:46.783760: Epoch   1 Batch  275/3125   train_loss = 1.027\n",
      "2022-02-11T23:12:47.605833: Epoch   1 Batch  295/3125   train_loss = 1.003\n",
      "2022-02-11T23:12:48.380531: Epoch   1 Batch  315/3125   train_loss = 1.025\n",
      "2022-02-11T23:12:49.215067: Epoch   1 Batch  335/3125   train_loss = 0.930\n",
      "2022-02-11T23:12:49.895449: Epoch   1 Batch  355/3125   train_loss = 1.126\n",
      "2022-02-11T23:12:50.773107: Epoch   1 Batch  375/3125   train_loss = 1.216\n",
      "2022-02-11T23:12:51.627223: Epoch   1 Batch  395/3125   train_loss = 1.068\n",
      "2022-02-11T23:12:52.464561: Epoch   1 Batch  415/3125   train_loss = 1.267\n",
      "2022-02-11T23:12:53.215860: Epoch   1 Batch  435/3125   train_loss = 1.074\n",
      "2022-02-11T23:12:53.941663: Epoch   1 Batch  455/3125   train_loss = 1.111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-11T23:12:54.690987: Epoch   1 Batch  475/3125   train_loss = 1.122\n",
      "2022-02-11T23:12:55.383123: Epoch   1 Batch  495/3125   train_loss = 1.044\n",
      "2022-02-11T23:12:56.209285: Epoch   1 Batch  515/3125   train_loss = 1.120\n",
      "2022-02-11T23:12:56.876678: Epoch   1 Batch  535/3125   train_loss = 1.194\n",
      "2022-02-11T23:12:57.686124: Epoch   1 Batch  555/3125   train_loss = 1.231\n",
      "2022-02-11T23:12:58.564469: Epoch   1 Batch  575/3125   train_loss = 1.172\n",
      "2022-02-11T23:12:59.418461: Epoch   1 Batch  595/3125   train_loss = 1.327\n",
      "2022-02-11T23:13:00.282387: Epoch   1 Batch  615/3125   train_loss = 1.079\n",
      "2022-02-11T23:13:01.101268: Epoch   1 Batch  635/3125   train_loss = 1.194\n",
      "2022-02-11T23:13:01.928247: Epoch   1 Batch  655/3125   train_loss = 1.010\n",
      "2022-02-11T23:13:02.728866: Epoch   1 Batch  675/3125   train_loss = 0.880\n",
      "2022-02-11T23:13:03.605795: Epoch   1 Batch  695/3125   train_loss = 1.079\n",
      "2022-02-11T23:13:04.450499: Epoch   1 Batch  715/3125   train_loss = 1.092\n",
      "2022-02-11T23:13:05.304199: Epoch   1 Batch  735/3125   train_loss = 0.997\n",
      "2022-02-11T23:13:06.142923: Epoch   1 Batch  755/3125   train_loss = 1.182\n",
      "2022-02-11T23:13:06.971582: Epoch   1 Batch  775/3125   train_loss = 1.006\n",
      "2022-02-11T23:13:07.838190: Epoch   1 Batch  795/3125   train_loss = 1.162\n",
      "2022-02-11T23:13:08.697580: Epoch   1 Batch  815/3125   train_loss = 1.055\n",
      "2022-02-11T23:13:09.560533: Epoch   1 Batch  835/3125   train_loss = 1.076\n",
      "2022-02-11T23:13:10.425355: Epoch   1 Batch  855/3125   train_loss = 1.267\n",
      "2022-02-11T23:13:11.211889: Epoch   1 Batch  875/3125   train_loss = 1.158\n",
      "2022-02-11T23:13:12.042843: Epoch   1 Batch  895/3125   train_loss = 1.056\n",
      "2022-02-11T23:13:12.927472: Epoch   1 Batch  915/3125   train_loss = 1.116\n",
      "2022-02-11T23:13:13.809696: Epoch   1 Batch  935/3125   train_loss = 1.167\n",
      "2022-02-11T23:13:14.661939: Epoch   1 Batch  955/3125   train_loss = 1.106\n",
      "2022-02-11T23:13:15.519157: Epoch   1 Batch  975/3125   train_loss = 1.134\n",
      "2022-02-11T23:13:16.356614: Epoch   1 Batch  995/3125   train_loss = 0.925\n",
      "2022-02-11T23:13:17.139118: Epoch   1 Batch 1015/3125   train_loss = 1.149\n",
      "2022-02-11T23:13:18.010799: Epoch   1 Batch 1035/3125   train_loss = 1.053\n",
      "2022-02-11T23:13:18.866778: Epoch   1 Batch 1055/3125   train_loss = 1.144\n",
      "2022-02-11T23:13:19.681759: Epoch   1 Batch 1075/3125   train_loss = 1.097\n",
      "2022-02-11T23:13:20.531805: Epoch   1 Batch 1095/3125   train_loss = 0.962\n",
      "2022-02-11T23:13:21.356170: Epoch   1 Batch 1115/3125   train_loss = 1.130\n",
      "2022-02-11T23:13:22.247821: Epoch   1 Batch 1135/3125   train_loss = 1.019\n",
      "2022-02-11T23:13:23.090206: Epoch   1 Batch 1155/3125   train_loss = 1.141\n",
      "2022-02-11T23:13:23.917803: Epoch   1 Batch 1175/3125   train_loss = 1.089\n",
      "2022-02-11T23:13:24.783371: Epoch   1 Batch 1195/3125   train_loss = 1.280\n",
      "2022-02-11T23:13:25.575929: Epoch   1 Batch 1215/3125   train_loss = 0.915\n",
      "2022-02-11T23:13:26.397626: Epoch   1 Batch 1235/3125   train_loss = 1.074\n",
      "2022-02-11T23:13:27.229252: Epoch   1 Batch 1255/3125   train_loss = 0.980\n",
      "2022-02-11T23:13:28.097579: Epoch   1 Batch 1275/3125   train_loss = 1.059\n",
      "2022-02-11T23:13:28.965070: Epoch   1 Batch 1295/3125   train_loss = 1.052\n",
      "2022-02-11T23:13:29.838090: Epoch   1 Batch 1315/3125   train_loss = 1.260\n",
      "2022-02-11T23:13:30.642568: Epoch   1 Batch 1335/3125   train_loss = 1.032\n",
      "2022-02-11T23:13:31.537008: Epoch   1 Batch 1355/3125   train_loss = 1.077\n",
      "2022-02-11T23:13:32.246239: Epoch   1 Batch 1375/3125   train_loss = 1.117\n",
      "2022-02-11T23:13:33.086367: Epoch   1 Batch 1395/3125   train_loss = 1.117\n",
      "2022-02-11T23:13:33.881535: Epoch   1 Batch 1415/3125   train_loss = 1.094\n",
      "2022-02-11T23:13:34.651945: Epoch   1 Batch 1435/3125   train_loss = 1.208\n",
      "2022-02-11T23:13:35.482824: Epoch   1 Batch 1455/3125   train_loss = 1.220\n",
      "2022-02-11T23:13:36.283530: Epoch   1 Batch 1475/3125   train_loss = 1.094\n",
      "2022-02-11T23:13:37.043282: Epoch   1 Batch 1495/3125   train_loss = 1.057\n",
      "2022-02-11T23:13:37.888986: Epoch   1 Batch 1515/3125   train_loss = 0.958\n",
      "2022-02-11T23:13:38.743015: Epoch   1 Batch 1535/3125   train_loss = 0.965\n",
      "2022-02-11T23:13:39.568041: Epoch   1 Batch 1555/3125   train_loss = 1.066\n",
      "2022-02-11T23:13:40.444613: Epoch   1 Batch 1575/3125   train_loss = 1.063\n",
      "2022-02-11T23:13:41.186958: Epoch   1 Batch 1595/3125   train_loss = 1.072\n",
      "2022-02-11T23:13:41.928115: Epoch   1 Batch 1615/3125   train_loss = 1.025\n",
      "2022-02-11T23:13:42.773615: Epoch   1 Batch 1635/3125   train_loss = 1.093\n",
      "2022-02-11T23:13:43.598480: Epoch   1 Batch 1655/3125   train_loss = 1.168\n",
      "2022-02-11T23:13:44.345221: Epoch   1 Batch 1675/3125   train_loss = 0.981\n",
      "2022-02-11T23:13:45.067648: Epoch   1 Batch 1695/3125   train_loss = 1.103\n",
      "2022-02-11T23:13:45.807360: Epoch   1 Batch 1715/3125   train_loss = 0.997\n",
      "2022-02-11T23:13:46.690476: Epoch   1 Batch 1735/3125   train_loss = 1.151\n",
      "2022-02-11T23:13:47.573826: Epoch   1 Batch 1755/3125   train_loss = 1.025\n",
      "2022-02-11T23:13:48.408286: Epoch   1 Batch 1775/3125   train_loss = 1.041\n",
      "2022-02-11T23:13:49.275064: Epoch   1 Batch 1795/3125   train_loss = 0.993\n",
      "2022-02-11T23:13:50.159523: Epoch   1 Batch 1815/3125   train_loss = 1.020\n",
      "2022-02-11T23:13:50.977029: Epoch   1 Batch 1835/3125   train_loss = 1.167\n",
      "2022-02-11T23:13:51.799803: Epoch   1 Batch 1855/3125   train_loss = 0.946\n",
      "2022-02-11T23:13:52.625120: Epoch   1 Batch 1875/3125   train_loss = 1.138\n",
      "2022-02-11T23:13:53.416142: Epoch   1 Batch 1895/3125   train_loss = 0.972\n",
      "2022-02-11T23:13:54.146673: Epoch   1 Batch 1915/3125   train_loss = 0.900\n",
      "2022-02-11T23:13:54.862608: Epoch   1 Batch 1935/3125   train_loss = 1.054\n",
      "2022-02-11T23:13:55.564733: Epoch   1 Batch 1955/3125   train_loss = 0.947\n",
      "2022-02-11T23:13:56.314661: Epoch   1 Batch 1975/3125   train_loss = 1.042\n",
      "2022-02-11T23:13:57.114926: Epoch   1 Batch 1995/3125   train_loss = 1.089\n",
      "2022-02-11T23:13:58.021542: Epoch   1 Batch 2015/3125   train_loss = 1.055\n",
      "2022-02-11T23:13:58.869175: Epoch   1 Batch 2035/3125   train_loss = 1.141\n",
      "2022-02-11T23:13:59.691335: Epoch   1 Batch 2055/3125   train_loss = 0.988\n",
      "2022-02-11T23:14:00.379071: Epoch   1 Batch 2075/3125   train_loss = 1.152\n",
      "2022-02-11T23:14:01.187266: Epoch   1 Batch 2095/3125   train_loss = 0.914\n",
      "2022-02-11T23:14:02.010557: Epoch   1 Batch 2115/3125   train_loss = 1.090\n",
      "2022-02-11T23:14:02.756779: Epoch   1 Batch 2135/3125   train_loss = 1.070\n",
      "2022-02-11T23:14:03.551803: Epoch   1 Batch 2155/3125   train_loss = 1.047\n",
      "2022-02-11T23:14:04.258237: Epoch   1 Batch 2175/3125   train_loss = 1.106\n",
      "2022-02-11T23:14:05.034072: Epoch   1 Batch 2195/3125   train_loss = 1.085\n",
      "2022-02-11T23:14:05.852791: Epoch   1 Batch 2215/3125   train_loss = 1.055\n",
      "2022-02-11T23:14:06.710436: Epoch   1 Batch 2235/3125   train_loss = 1.144\n",
      "2022-02-11T23:14:07.429007: Epoch   1 Batch 2255/3125   train_loss = 1.203\n",
      "2022-02-11T23:14:08.169362: Epoch   1 Batch 2275/3125   train_loss = 0.984\n",
      "2022-02-11T23:14:08.874594: Epoch   1 Batch 2295/3125   train_loss = 1.268\n",
      "2022-02-11T23:14:09.652612: Epoch   1 Batch 2315/3125   train_loss = 1.136\n",
      "2022-02-11T23:14:10.468890: Epoch   1 Batch 2335/3125   train_loss = 1.035\n",
      "2022-02-11T23:14:11.101346: Epoch   1 Batch 2355/3125   train_loss = 1.049\n",
      "2022-02-11T23:14:11.691135: Epoch   1 Batch 2375/3125   train_loss = 1.251\n",
      "2022-02-11T23:14:12.305933: Epoch   1 Batch 2395/3125   train_loss = 1.041\n",
      "2022-02-11T23:14:12.964134: Epoch   1 Batch 2415/3125   train_loss = 1.052\n",
      "2022-02-11T23:14:13.794859: Epoch   1 Batch 2435/3125   train_loss = 0.947\n",
      "2022-02-11T23:14:14.622882: Epoch   1 Batch 2455/3125   train_loss = 1.080\n",
      "2022-02-11T23:14:15.397675: Epoch   1 Batch 2475/3125   train_loss = 0.997\n",
      "2022-02-11T23:14:16.168747: Epoch   1 Batch 2495/3125   train_loss = 0.999\n",
      "2022-02-11T23:14:17.036298: Epoch   1 Batch 2515/3125   train_loss = 1.031\n",
      "2022-02-11T23:14:17.803845: Epoch   1 Batch 2535/3125   train_loss = 1.005\n",
      "2022-02-11T23:14:18.511535: Epoch   1 Batch 2555/3125   train_loss = 0.925\n",
      "2022-02-11T23:14:19.300213: Epoch   1 Batch 2575/3125   train_loss = 0.926\n",
      "2022-02-11T23:14:20.095078: Epoch   1 Batch 2595/3125   train_loss = 1.012\n",
      "2022-02-11T23:14:20.936511: Epoch   1 Batch 2615/3125   train_loss = 1.128\n",
      "2022-02-11T23:14:21.707886: Epoch   1 Batch 2635/3125   train_loss = 0.960\n",
      "2022-02-11T23:14:22.513754: Epoch   1 Batch 2655/3125   train_loss = 1.015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-11T23:14:23.220569: Epoch   1 Batch 2675/3125   train_loss = 0.977\n",
      "2022-02-11T23:14:23.963044: Epoch   1 Batch 2695/3125   train_loss = 1.036\n",
      "2022-02-11T23:14:24.720352: Epoch   1 Batch 2715/3125   train_loss = 1.000\n",
      "2022-02-11T23:14:25.541418: Epoch   1 Batch 2735/3125   train_loss = 0.909\n",
      "2022-02-11T23:14:26.334590: Epoch   1 Batch 2755/3125   train_loss = 1.018\n",
      "2022-02-11T23:14:27.164514: Epoch   1 Batch 2775/3125   train_loss = 1.131\n",
      "2022-02-11T23:14:27.966388: Epoch   1 Batch 2795/3125   train_loss = 1.058\n",
      "2022-02-11T23:14:28.721631: Epoch   1 Batch 2815/3125   train_loss = 0.958\n",
      "2022-02-11T23:14:29.364779: Epoch   1 Batch 2835/3125   train_loss = 1.123\n",
      "2022-02-11T23:14:30.178111: Epoch   1 Batch 2855/3125   train_loss = 1.071\n",
      "2022-02-11T23:14:30.860849: Epoch   1 Batch 2875/3125   train_loss = 1.023\n",
      "2022-02-11T23:14:31.639588: Epoch   1 Batch 2895/3125   train_loss = 1.099\n",
      "2022-02-11T23:14:32.502928: Epoch   1 Batch 2915/3125   train_loss = 0.990\n",
      "2022-02-11T23:14:33.328490: Epoch   1 Batch 2935/3125   train_loss = 1.064\n",
      "2022-02-11T23:14:34.115069: Epoch   1 Batch 2955/3125   train_loss = 1.052\n",
      "2022-02-11T23:14:34.956819: Epoch   1 Batch 2975/3125   train_loss = 1.092\n",
      "2022-02-11T23:14:35.799941: Epoch   1 Batch 2995/3125   train_loss = 0.928\n",
      "2022-02-11T23:14:36.672913: Epoch   1 Batch 3015/3125   train_loss = 1.011\n",
      "2022-02-11T23:14:37.411448: Epoch   1 Batch 3035/3125   train_loss = 1.051\n",
      "2022-02-11T23:14:38.227223: Epoch   1 Batch 3055/3125   train_loss = 1.128\n",
      "2022-02-11T23:14:39.044429: Epoch   1 Batch 3075/3125   train_loss = 1.035\n",
      "2022-02-11T23:14:39.806711: Epoch   1 Batch 3095/3125   train_loss = 1.007\n",
      "2022-02-11T23:14:40.693387: Epoch   1 Batch 3115/3125   train_loss = 0.890\n",
      "2022-02-11T23:14:41.264244: Epoch   1 Batch   19/781   test_loss = 1.016\n",
      "2022-02-11T23:14:41.510573: Epoch   1 Batch   39/781   test_loss = 0.846\n",
      "2022-02-11T23:14:41.746816: Epoch   1 Batch   59/781   test_loss = 0.922\n",
      "2022-02-11T23:14:41.967472: Epoch   1 Batch   79/781   test_loss = 1.093\n",
      "2022-02-11T23:14:42.240058: Epoch   1 Batch   99/781   test_loss = 1.024\n",
      "2022-02-11T23:14:42.485212: Epoch   1 Batch  119/781   test_loss = 0.913\n",
      "2022-02-11T23:14:42.715509: Epoch   1 Batch  139/781   test_loss = 1.047\n",
      "2022-02-11T23:14:42.924383: Epoch   1 Batch  159/781   test_loss = 1.040\n",
      "2022-02-11T23:14:43.192397: Epoch   1 Batch  179/781   test_loss = 1.060\n",
      "2022-02-11T23:14:43.446991: Epoch   1 Batch  199/781   test_loss = 0.890\n",
      "2022-02-11T23:14:43.643910: Epoch   1 Batch  219/781   test_loss = 1.092\n",
      "2022-02-11T23:14:43.866351: Epoch   1 Batch  239/781   test_loss = 1.182\n",
      "2022-02-11T23:14:44.114486: Epoch   1 Batch  259/781   test_loss = 0.974\n",
      "2022-02-11T23:14:44.360819: Epoch   1 Batch  279/781   test_loss = 1.117\n",
      "2022-02-11T23:14:44.626832: Epoch   1 Batch  299/781   test_loss = 1.169\n",
      "2022-02-11T23:14:44.856596: Epoch   1 Batch  319/781   test_loss = 0.955\n",
      "2022-02-11T23:14:45.113620: Epoch   1 Batch  339/781   test_loss = 0.904\n",
      "2022-02-11T23:14:45.382791: Epoch   1 Batch  359/781   test_loss = 0.966\n",
      "2022-02-11T23:14:45.626120: Epoch   1 Batch  379/781   test_loss = 1.058\n",
      "2022-02-11T23:14:45.822219: Epoch   1 Batch  399/781   test_loss = 0.869\n",
      "2022-02-11T23:14:46.071353: Epoch   1 Batch  419/781   test_loss = 0.968\n",
      "2022-02-11T23:14:46.328467: Epoch   1 Batch  439/781   test_loss = 1.069\n",
      "2022-02-11T23:14:46.576811: Epoch   1 Batch  459/781   test_loss = 1.059\n",
      "2022-02-11T23:14:46.783125: Epoch   1 Batch  479/781   test_loss = 1.052\n",
      "2022-02-11T23:14:47.012735: Epoch   1 Batch  499/781   test_loss = 0.978\n",
      "2022-02-11T23:14:47.264290: Epoch   1 Batch  519/781   test_loss = 1.056\n",
      "2022-02-11T23:14:47.521674: Epoch   1 Batch  539/781   test_loss = 0.899\n",
      "2022-02-11T23:14:47.770428: Epoch   1 Batch  559/781   test_loss = 1.106\n",
      "2022-02-11T23:14:47.959768: Epoch   1 Batch  579/781   test_loss = 0.993\n",
      "2022-02-11T23:14:48.203912: Epoch   1 Batch  599/781   test_loss = 0.983\n",
      "2022-02-11T23:14:48.451085: Epoch   1 Batch  619/781   test_loss = 1.187\n",
      "2022-02-11T23:14:48.708216: Epoch   1 Batch  639/781   test_loss = 0.912\n",
      "2022-02-11T23:14:48.915853: Epoch   1 Batch  659/781   test_loss = 1.169\n",
      "2022-02-11T23:14:49.154332: Epoch   1 Batch  679/781   test_loss = 1.121\n",
      "2022-02-11T23:14:49.406238: Epoch   1 Batch  699/781   test_loss = 0.837\n",
      "2022-02-11T23:14:49.650112: Epoch   1 Batch  719/781   test_loss = 0.974\n",
      "2022-02-11T23:14:49.856268: Epoch   1 Batch  739/781   test_loss = 0.999\n",
      "2022-02-11T23:14:50.071176: Epoch   1 Batch  759/781   test_loss = 0.890\n",
      "2022-02-11T23:14:50.311252: Epoch   1 Batch  779/781   test_loss = 0.837\n",
      "2022-02-11T23:14:51.119957: Epoch   2 Batch   10/3125   train_loss = 0.958\n",
      "2022-02-11T23:14:51.876229: Epoch   2 Batch   30/3125   train_loss = 1.031\n",
      "2022-02-11T23:14:52.566749: Epoch   2 Batch   50/3125   train_loss = 1.119\n",
      "2022-02-11T23:14:53.359634: Epoch   2 Batch   70/3125   train_loss = 1.145\n",
      "2022-02-11T23:14:54.203721: Epoch   2 Batch   90/3125   train_loss = 1.048\n",
      "2022-02-11T23:14:55.006169: Epoch   2 Batch  110/3125   train_loss = 0.859\n",
      "2022-02-11T23:14:55.844662: Epoch   2 Batch  130/3125   train_loss = 0.962\n",
      "2022-02-11T23:14:56.716409: Epoch   2 Batch  150/3125   train_loss = 1.115\n",
      "2022-02-11T23:14:57.562457: Epoch   2 Batch  170/3125   train_loss = 0.997\n",
      "2022-02-11T23:14:58.429160: Epoch   2 Batch  190/3125   train_loss = 1.035\n",
      "2022-02-11T23:14:59.183648: Epoch   2 Batch  210/3125   train_loss = 0.964\n",
      "2022-02-11T23:14:59.972654: Epoch   2 Batch  230/3125   train_loss = 1.041\n",
      "2022-02-11T23:15:00.661234: Epoch   2 Batch  250/3125   train_loss = 0.901\n",
      "2022-02-11T23:15:01.401980: Epoch   2 Batch  270/3125   train_loss = 0.816\n",
      "2022-02-11T23:15:02.251350: Epoch   2 Batch  290/3125   train_loss = 1.010\n",
      "2022-02-11T23:15:03.088318: Epoch   2 Batch  310/3125   train_loss = 0.954\n",
      "2022-02-11T23:15:03.923973: Epoch   2 Batch  330/3125   train_loss = 1.048\n",
      "2022-02-11T23:15:04.707718: Epoch   2 Batch  350/3125   train_loss = 0.956\n",
      "2022-02-11T23:15:05.348483: Epoch   2 Batch  370/3125   train_loss = 1.159\n",
      "2022-02-11T23:15:06.188330: Epoch   2 Batch  390/3125   train_loss = 1.137\n",
      "2022-02-11T23:15:07.075340: Epoch   2 Batch  410/3125   train_loss = 0.905\n",
      "2022-02-11T23:15:07.828722: Epoch   2 Batch  430/3125   train_loss = 1.177\n",
      "2022-02-11T23:15:08.451750: Epoch   2 Batch  450/3125   train_loss = 0.960\n",
      "2022-02-11T23:15:09.226459: Epoch   2 Batch  470/3125   train_loss = 1.003\n",
      "2022-02-11T23:15:09.978469: Epoch   2 Batch  490/3125   train_loss = 0.986\n",
      "2022-02-11T23:15:10.785164: Epoch   2 Batch  510/3125   train_loss = 1.060\n",
      "2022-02-11T23:15:11.616848: Epoch   2 Batch  530/3125   train_loss = 0.985\n",
      "2022-02-11T23:15:12.453511: Epoch   2 Batch  550/3125   train_loss = 1.027\n",
      "2022-02-11T23:15:13.332751: Epoch   2 Batch  570/3125   train_loss = 1.113\n",
      "2022-02-11T23:15:14.066305: Epoch   2 Batch  590/3125   train_loss = 0.999\n",
      "2022-02-11T23:15:14.737240: Epoch   2 Batch  610/3125   train_loss = 0.923\n",
      "2022-02-11T23:15:15.450197: Epoch   2 Batch  630/3125   train_loss = 1.090\n",
      "2022-02-11T23:15:16.213233: Epoch   2 Batch  650/3125   train_loss = 0.983\n",
      "2022-02-11T23:15:17.040040: Epoch   2 Batch  670/3125   train_loss = 0.896\n",
      "2022-02-11T23:15:17.739737: Epoch   2 Batch  690/3125   train_loss = 0.967\n",
      "2022-02-11T23:15:18.553759: Epoch   2 Batch  710/3125   train_loss = 1.004\n",
      "2022-02-11T23:15:19.287132: Epoch   2 Batch  730/3125   train_loss = 0.824\n",
      "2022-02-11T23:15:20.123898: Epoch   2 Batch  750/3125   train_loss = 0.989\n",
      "2022-02-11T23:15:20.914050: Epoch   2 Batch  770/3125   train_loss = 0.933\n",
      "2022-02-11T23:15:21.777199: Epoch   2 Batch  790/3125   train_loss = 0.909\n",
      "2022-02-11T23:15:22.606850: Epoch   2 Batch  810/3125   train_loss = 0.879\n",
      "2022-02-11T23:15:23.489378: Epoch   2 Batch  830/3125   train_loss = 0.868\n",
      "2022-02-11T23:15:24.291985: Epoch   2 Batch  850/3125   train_loss = 1.042\n",
      "2022-02-11T23:15:25.135124: Epoch   2 Batch  870/3125   train_loss = 0.856\n",
      "2022-02-11T23:15:25.824194: Epoch   2 Batch  890/3125   train_loss = 0.918\n",
      "2022-02-11T23:15:26.615638: Epoch   2 Batch  910/3125   train_loss = 1.025\n",
      "2022-02-11T23:15:27.441308: Epoch   2 Batch  930/3125   train_loss = 1.056\n",
      "2022-02-11T23:15:28.237545: Epoch   2 Batch  950/3125   train_loss = 0.954\n",
      "2022-02-11T23:15:29.054680: Epoch   2 Batch  970/3125   train_loss = 1.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-11T23:15:29.808064: Epoch   2 Batch  990/3125   train_loss = 0.867\n",
      "2022-02-11T23:15:30.512818: Epoch   2 Batch 1010/3125   train_loss = 1.120\n",
      "2022-02-11T23:15:31.376311: Epoch   2 Batch 1030/3125   train_loss = 0.969\n",
      "2022-02-11T23:15:32.219664: Epoch   2 Batch 1050/3125   train_loss = 0.838\n",
      "2022-02-11T23:15:33.063218: Epoch   2 Batch 1070/3125   train_loss = 0.975\n",
      "2022-02-11T23:15:33.911382: Epoch   2 Batch 1090/3125   train_loss = 1.082\n",
      "2022-02-11T23:15:34.782214: Epoch   2 Batch 1110/3125   train_loss = 1.065\n",
      "2022-02-11T23:15:35.567977: Epoch   2 Batch 1130/3125   train_loss = 1.002\n",
      "2022-02-11T23:15:36.245024: Epoch   2 Batch 1150/3125   train_loss = 0.950\n",
      "2022-02-11T23:15:37.088373: Epoch   2 Batch 1170/3125   train_loss = 0.951\n",
      "2022-02-11T23:15:37.869603: Epoch   2 Batch 1190/3125   train_loss = 1.021\n",
      "2022-02-11T23:15:38.561304: Epoch   2 Batch 1210/3125   train_loss = 0.862\n",
      "2022-02-11T23:15:39.325327: Epoch   2 Batch 1230/3125   train_loss = 0.908\n",
      "2022-02-11T23:15:40.103426: Epoch   2 Batch 1250/3125   train_loss = 0.966\n",
      "2022-02-11T23:15:40.864480: Epoch   2 Batch 1270/3125   train_loss = 0.978\n",
      "2022-02-11T23:15:41.663050: Epoch   2 Batch 1290/3125   train_loss = 0.965\n",
      "2022-02-11T23:15:42.472437: Epoch   2 Batch 1310/3125   train_loss = 0.997\n",
      "2022-02-11T23:15:43.305975: Epoch   2 Batch 1330/3125   train_loss = 1.138\n",
      "2022-02-11T23:15:44.086076: Epoch   2 Batch 1350/3125   train_loss = 0.933\n",
      "2022-02-11T23:15:44.803833: Epoch   2 Batch 1370/3125   train_loss = 0.869\n",
      "2022-02-11T23:15:45.646043: Epoch   2 Batch 1390/3125   train_loss = 0.995\n",
      "2022-02-11T23:15:46.424065: Epoch   2 Batch 1410/3125   train_loss = 0.964\n",
      "2022-02-11T23:15:47.165520: Epoch   2 Batch 1430/3125   train_loss = 1.002\n",
      "2022-02-11T23:15:47.934728: Epoch   2 Batch 1450/3125   train_loss = 1.027\n",
      "2022-02-11T23:15:48.675955: Epoch   2 Batch 1470/3125   train_loss = 1.009\n",
      "2022-02-11T23:15:49.493214: Epoch   2 Batch 1490/3125   train_loss = 1.019\n",
      "2022-02-11T23:15:50.343671: Epoch   2 Batch 1510/3125   train_loss = 0.961\n",
      "2022-02-11T23:15:51.094258: Epoch   2 Batch 1530/3125   train_loss = 1.077\n",
      "2022-02-11T23:15:51.759480: Epoch   2 Batch 1550/3125   train_loss = 0.890\n",
      "2022-02-11T23:15:52.555236: Epoch   2 Batch 1570/3125   train_loss = 0.958\n",
      "2022-02-11T23:15:53.420139: Epoch   2 Batch 1590/3125   train_loss = 0.947\n",
      "2022-02-11T23:15:54.285499: Epoch   2 Batch 1610/3125   train_loss = 1.008\n",
      "2022-02-11T23:15:55.160551: Epoch   2 Batch 1630/3125   train_loss = 1.071\n",
      "2022-02-11T23:15:56.043026: Epoch   2 Batch 1650/3125   train_loss = 0.855\n",
      "2022-02-11T23:15:56.886391: Epoch   2 Batch 1670/3125   train_loss = 0.852\n",
      "2022-02-11T23:15:57.725688: Epoch   2 Batch 1690/3125   train_loss = 1.008\n",
      "2022-02-11T23:15:58.571726: Epoch   2 Batch 1710/3125   train_loss = 0.940\n",
      "2022-02-11T23:15:59.365615: Epoch   2 Batch 1730/3125   train_loss = 1.025\n",
      "2022-02-11T23:16:00.205589: Epoch   2 Batch 1750/3125   train_loss = 0.882\n",
      "2022-02-11T23:16:00.985247: Epoch   2 Batch 1770/3125   train_loss = 1.154\n",
      "2022-02-11T23:16:01.799115: Epoch   2 Batch 1790/3125   train_loss = 1.054\n",
      "2022-02-11T23:16:02.641018: Epoch   2 Batch 1810/3125   train_loss = 0.983\n",
      "2022-02-11T23:16:03.484665: Epoch   2 Batch 1830/3125   train_loss = 1.090\n",
      "2022-02-11T23:16:04.345436: Epoch   2 Batch 1850/3125   train_loss = 0.971\n",
      "2022-02-11T23:16:05.051684: Epoch   2 Batch 1870/3125   train_loss = 1.037\n",
      "2022-02-11T23:16:05.691106: Epoch   2 Batch 1890/3125   train_loss = 0.829\n",
      "2022-02-11T23:16:06.413934: Epoch   2 Batch 1910/3125   train_loss = 0.941\n",
      "2022-02-11T23:16:07.253818: Epoch   2 Batch 1930/3125   train_loss = 0.985\n",
      "2022-02-11T23:16:08.100505: Epoch   2 Batch 1950/3125   train_loss = 0.868\n",
      "2022-02-11T23:16:08.913620: Epoch   2 Batch 1970/3125   train_loss = 1.020\n",
      "2022-02-11T23:16:09.766490: Epoch   2 Batch 1990/3125   train_loss = 0.865\n",
      "2022-02-11T23:16:10.651154: Epoch   2 Batch 2010/3125   train_loss = 0.823\n",
      "2022-02-11T23:16:11.502036: Epoch   2 Batch 2030/3125   train_loss = 0.914\n",
      "2022-02-11T23:16:12.289889: Epoch   2 Batch 2050/3125   train_loss = 0.915\n",
      "2022-02-11T23:16:13.121939: Epoch   2 Batch 2070/3125   train_loss = 0.926\n",
      "2022-02-11T23:16:13.910110: Epoch   2 Batch 2090/3125   train_loss = 0.872\n",
      "2022-02-11T23:16:14.746772: Epoch   2 Batch 2110/3125   train_loss = 0.961\n",
      "2022-02-11T23:16:15.523019: Epoch   2 Batch 2130/3125   train_loss = 0.949\n",
      "2022-02-11T23:16:16.330503: Epoch   2 Batch 2150/3125   train_loss = 0.996\n",
      "2022-02-11T23:16:17.072155: Epoch   2 Batch 2170/3125   train_loss = 0.830\n",
      "2022-02-11T23:16:17.806840: Epoch   2 Batch 2190/3125   train_loss = 0.951\n",
      "2022-02-11T23:16:18.505418: Epoch   2 Batch 2210/3125   train_loss = 0.984\n",
      "2022-02-11T23:16:19.344717: Epoch   2 Batch 2230/3125   train_loss = 0.848\n",
      "2022-02-11T23:16:20.136523: Epoch   2 Batch 2250/3125   train_loss = 1.056\n",
      "2022-02-11T23:16:20.996439: Epoch   2 Batch 2270/3125   train_loss = 0.928\n",
      "2022-02-11T23:16:21.737099: Epoch   2 Batch 2290/3125   train_loss = 0.828\n",
      "2022-02-11T23:16:22.618565: Epoch   2 Batch 2310/3125   train_loss = 0.883\n",
      "2022-02-11T23:16:23.419908: Epoch   2 Batch 2330/3125   train_loss = 1.049\n",
      "2022-02-11T23:16:24.251101: Epoch   2 Batch 2350/3125   train_loss = 1.063\n",
      "2022-02-11T23:16:25.027931: Epoch   2 Batch 2370/3125   train_loss = 0.925\n",
      "2022-02-11T23:16:25.850197: Epoch   2 Batch 2390/3125   train_loss = 1.065\n",
      "2022-02-11T23:16:26.664059: Epoch   2 Batch 2410/3125   train_loss = 1.066\n",
      "2022-02-11T23:16:27.483728: Epoch   2 Batch 2430/3125   train_loss = 0.941\n",
      "2022-02-11T23:16:28.269582: Epoch   2 Batch 2450/3125   train_loss = 0.956\n",
      "2022-02-11T23:16:29.068306: Epoch   2 Batch 2470/3125   train_loss = 0.972\n",
      "2022-02-11T23:16:29.932968: Epoch   2 Batch 2490/3125   train_loss = 1.101\n",
      "2022-02-11T23:16:30.744196: Epoch   2 Batch 2510/3125   train_loss = 1.098\n",
      "2022-02-11T23:16:31.521394: Epoch   2 Batch 2530/3125   train_loss = 0.844\n",
      "2022-02-11T23:16:32.232416: Epoch   2 Batch 2550/3125   train_loss = 1.042\n",
      "2022-02-11T23:16:32.972918: Epoch   2 Batch 2570/3125   train_loss = 1.010\n",
      "2022-02-11T23:16:33.763107: Epoch   2 Batch 2590/3125   train_loss = 1.012\n",
      "2022-02-11T23:16:34.567879: Epoch   2 Batch 2610/3125   train_loss = 1.050\n",
      "2022-02-11T23:16:35.387435: Epoch   2 Batch 2630/3125   train_loss = 0.674\n",
      "2022-02-11T23:16:36.258246: Epoch   2 Batch 2650/3125   train_loss = 0.959\n",
      "2022-02-11T23:16:37.081383: Epoch   2 Batch 2670/3125   train_loss = 0.982\n",
      "2022-02-11T23:16:37.907216: Epoch   2 Batch 2690/3125   train_loss = 0.960\n",
      "2022-02-11T23:16:38.614714: Epoch   2 Batch 2710/3125   train_loss = 0.862\n",
      "2022-02-11T23:16:39.362412: Epoch   2 Batch 2730/3125   train_loss = 1.082\n",
      "2022-02-11T23:16:40.032511: Epoch   2 Batch 2750/3125   train_loss = 1.024\n",
      "2022-02-11T23:16:40.719833: Epoch   2 Batch 2770/3125   train_loss = 0.942\n",
      "2022-02-11T23:16:41.356073: Epoch   2 Batch 2790/3125   train_loss = 0.873\n",
      "2022-02-11T23:16:42.172791: Epoch   2 Batch 2810/3125   train_loss = 0.939\n",
      "2022-02-11T23:16:43.057303: Epoch   2 Batch 2830/3125   train_loss = 0.887\n",
      "2022-02-11T23:16:43.905278: Epoch   2 Batch 2850/3125   train_loss = 1.034\n",
      "2022-02-11T23:16:44.767392: Epoch   2 Batch 2870/3125   train_loss = 0.811\n",
      "2022-02-11T23:16:45.581540: Epoch   2 Batch 2890/3125   train_loss = 0.826\n",
      "2022-02-11T23:16:46.382591: Epoch   2 Batch 2910/3125   train_loss = 0.970\n",
      "2022-02-11T23:16:47.122108: Epoch   2 Batch 2930/3125   train_loss = 0.871\n",
      "2022-02-11T23:16:47.819162: Epoch   2 Batch 2950/3125   train_loss = 1.052\n",
      "2022-02-11T23:16:48.583990: Epoch   2 Batch 2970/3125   train_loss = 0.967\n",
      "2022-02-11T23:16:49.417912: Epoch   2 Batch 2990/3125   train_loss = 0.946\n",
      "2022-02-11T23:16:50.256071: Epoch   2 Batch 3010/3125   train_loss = 0.970\n",
      "2022-02-11T23:16:51.096512: Epoch   2 Batch 3030/3125   train_loss = 0.961\n",
      "2022-02-11T23:16:51.892404: Epoch   2 Batch 3050/3125   train_loss = 0.933\n",
      "2022-02-11T23:16:52.648995: Epoch   2 Batch 3070/3125   train_loss = 0.849\n",
      "2022-02-11T23:16:53.336190: Epoch   2 Batch 3090/3125   train_loss = 0.819\n",
      "2022-02-11T23:16:54.003509: Epoch   2 Batch 3110/3125   train_loss = 0.826\n",
      "2022-02-11T23:16:54.698732: Epoch   2 Batch   18/781   test_loss = 0.791\n",
      "2022-02-11T23:16:54.948316: Epoch   2 Batch   38/781   test_loss = 0.900\n",
      "2022-02-11T23:16:55.188915: Epoch   2 Batch   58/781   test_loss = 0.805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-11T23:16:55.439334: Epoch   2 Batch   78/781   test_loss = 0.936\n",
      "2022-02-11T23:16:55.633221: Epoch   2 Batch   98/781   test_loss = 0.977\n",
      "2022-02-11T23:16:55.848389: Epoch   2 Batch  118/781   test_loss = 0.852\n",
      "2022-02-11T23:16:56.090298: Epoch   2 Batch  138/781   test_loss = 0.995\n",
      "2022-02-11T23:16:56.399949: Epoch   2 Batch  158/781   test_loss = 0.830\n",
      "2022-02-11T23:16:56.655166: Epoch   2 Batch  178/781   test_loss = 0.848\n",
      "2022-02-11T23:16:56.887021: Epoch   2 Batch  198/781   test_loss = 0.880\n",
      "2022-02-11T23:16:57.105734: Epoch   2 Batch  218/781   test_loss = 1.024\n",
      "2022-02-11T23:16:57.327199: Epoch   2 Batch  238/781   test_loss = 0.960\n",
      "2022-02-11T23:16:57.572647: Epoch   2 Batch  258/781   test_loss = 1.038\n",
      "2022-02-11T23:16:57.828205: Epoch   2 Batch  278/781   test_loss = 1.084\n",
      "2022-02-11T23:16:58.041790: Epoch   2 Batch  298/781   test_loss = 0.887\n",
      "2022-02-11T23:16:58.248003: Epoch   2 Batch  318/781   test_loss = 0.909\n",
      "2022-02-11T23:16:58.473078: Epoch   2 Batch  338/781   test_loss = 0.877\n",
      "2022-02-11T23:16:58.687285: Epoch   2 Batch  358/781   test_loss = 0.932\n",
      "2022-02-11T23:16:58.949198: Epoch   2 Batch  378/781   test_loss = 0.855\n",
      "2022-02-11T23:16:59.188434: Epoch   2 Batch  398/781   test_loss = 0.849\n",
      "2022-02-11T23:16:59.397727: Epoch   2 Batch  418/781   test_loss = 0.956\n",
      "2022-02-11T23:16:59.622010: Epoch   2 Batch  438/781   test_loss = 1.004\n",
      "2022-02-11T23:16:59.839142: Epoch   2 Batch  458/781   test_loss = 0.896\n",
      "2022-02-11T23:17:00.083050: Epoch   2 Batch  478/781   test_loss = 0.939\n",
      "2022-02-11T23:17:00.381922: Epoch   2 Batch  498/781   test_loss = 0.832\n",
      "2022-02-11T23:17:00.674960: Epoch   2 Batch  518/781   test_loss = 0.886\n",
      "2022-02-11T23:17:00.861183: Epoch   2 Batch  538/781   test_loss = 0.834\n",
      "2022-02-11T23:17:01.075666: Epoch   2 Batch  558/781   test_loss = 0.885\n",
      "2022-02-11T23:17:01.288049: Epoch   2 Batch  578/781   test_loss = 0.887\n",
      "2022-02-11T23:17:01.485694: Epoch   2 Batch  598/781   test_loss = 1.057\n",
      "2022-02-11T23:17:01.679638: Epoch   2 Batch  618/781   test_loss = 0.868\n",
      "2022-02-11T23:17:01.875744: Epoch   2 Batch  638/781   test_loss = 0.883\n",
      "2022-02-11T23:17:02.080888: Epoch   2 Batch  658/781   test_loss = 0.985\n",
      "2022-02-11T23:17:02.281576: Epoch   2 Batch  678/781   test_loss = 0.938\n",
      "2022-02-11T23:17:02.481581: Epoch   2 Batch  698/781   test_loss = 0.888\n",
      "2022-02-11T23:17:02.687015: Epoch   2 Batch  718/781   test_loss = 1.029\n",
      "2022-02-11T23:17:02.920174: Epoch   2 Batch  738/781   test_loss = 0.892\n",
      "2022-02-11T23:17:03.128346: Epoch   2 Batch  758/781   test_loss = 0.935\n",
      "2022-02-11T23:17:03.335294: Epoch   2 Batch  778/781   test_loss = 0.909\n",
      "2022-02-11T23:17:04.082053: Epoch   3 Batch    5/3125   train_loss = 0.938\n",
      "2022-02-11T23:17:04.947337: Epoch   3 Batch   25/3125   train_loss = 1.001\n",
      "2022-02-11T23:17:05.823353: Epoch   3 Batch   45/3125   train_loss = 0.876\n",
      "2022-02-11T23:17:06.690859: Epoch   3 Batch   65/3125   train_loss = 0.933\n",
      "2022-02-11T23:17:07.535905: Epoch   3 Batch   85/3125   train_loss = 0.879\n",
      "2022-02-11T23:17:08.303966: Epoch   3 Batch  105/3125   train_loss = 0.730\n",
      "2022-02-11T23:17:09.139762: Epoch   3 Batch  125/3125   train_loss = 0.952\n",
      "2022-02-11T23:17:10.022424: Epoch   3 Batch  145/3125   train_loss = 0.908\n",
      "2022-02-11T23:17:10.839996: Epoch   3 Batch  165/3125   train_loss = 0.945\n",
      "2022-02-11T23:17:11.523774: Epoch   3 Batch  185/3125   train_loss = 0.832\n",
      "2022-02-11T23:17:12.364543: Epoch   3 Batch  205/3125   train_loss = 0.826\n",
      "2022-02-11T23:17:13.179615: Epoch   3 Batch  225/3125   train_loss = 0.806\n",
      "2022-02-11T23:17:14.006311: Epoch   3 Batch  245/3125   train_loss = 1.126\n",
      "2022-02-11T23:17:14.736188: Epoch   3 Batch  265/3125   train_loss = 0.915\n",
      "2022-02-11T23:17:15.471624: Epoch   3 Batch  285/3125   train_loss = 0.989\n",
      "2022-02-11T23:17:16.214296: Epoch   3 Batch  305/3125   train_loss = 0.874\n",
      "2022-02-11T23:17:16.993492: Epoch   3 Batch  325/3125   train_loss = 0.947\n",
      "2022-02-11T23:17:17.855300: Epoch   3 Batch  345/3125   train_loss = 0.952\n",
      "2022-02-11T23:17:18.584413: Epoch   3 Batch  365/3125   train_loss = 0.950\n",
      "2022-02-11T23:17:19.402028: Epoch   3 Batch  385/3125   train_loss = 0.855\n",
      "2022-02-11T23:17:20.182441: Epoch   3 Batch  405/3125   train_loss = 0.881\n",
      "2022-02-11T23:17:20.928750: Epoch   3 Batch  425/3125   train_loss = 0.921\n",
      "2022-02-11T23:17:21.769285: Epoch   3 Batch  445/3125   train_loss = 0.944\n",
      "2022-02-11T23:17:22.564148: Epoch   3 Batch  465/3125   train_loss = 0.874\n",
      "2022-02-11T23:17:23.384497: Epoch   3 Batch  485/3125   train_loss = 0.991\n",
      "2022-02-11T23:17:24.040488: Epoch   3 Batch  505/3125   train_loss = 0.888\n",
      "2022-02-11T23:17:24.826191: Epoch   3 Batch  525/3125   train_loss = 0.982\n",
      "2022-02-11T23:17:25.684927: Epoch   3 Batch  545/3125   train_loss = 0.831\n",
      "2022-02-11T23:17:26.526990: Epoch   3 Batch  565/3125   train_loss = 1.092\n",
      "2022-02-11T23:17:27.356236: Epoch   3 Batch  585/3125   train_loss = 0.886\n",
      "2022-02-11T23:17:28.136547: Epoch   3 Batch  605/3125   train_loss = 0.897\n",
      "2022-02-11T23:17:28.937882: Epoch   3 Batch  625/3125   train_loss = 0.931\n",
      "2022-02-11T23:17:29.615506: Epoch   3 Batch  645/3125   train_loss = 1.005\n",
      "2022-02-11T23:17:30.434781: Epoch   3 Batch  665/3125   train_loss = 1.012\n",
      "2022-02-11T23:17:31.250720: Epoch   3 Batch  685/3125   train_loss = 0.917\n",
      "2022-02-11T23:17:32.082850: Epoch   3 Batch  705/3125   train_loss = 0.988\n",
      "2022-02-11T23:17:32.859340: Epoch   3 Batch  725/3125   train_loss = 0.928\n",
      "2022-02-11T23:17:33.642199: Epoch   3 Batch  745/3125   train_loss = 0.901\n",
      "2022-02-11T23:17:34.540662: Epoch   3 Batch  765/3125   train_loss = 0.932\n",
      "2022-02-11T23:17:35.279663: Epoch   3 Batch  785/3125   train_loss = 1.093\n",
      "2022-02-11T23:17:36.031761: Epoch   3 Batch  805/3125   train_loss = 0.827\n",
      "2022-02-11T23:17:36.851410: Epoch   3 Batch  825/3125   train_loss = 0.903\n",
      "2022-02-11T23:17:37.662364: Epoch   3 Batch  845/3125   train_loss = 0.941\n",
      "2022-02-11T23:17:38.491220: Epoch   3 Batch  865/3125   train_loss = 1.011\n",
      "2022-02-11T23:17:39.274118: Epoch   3 Batch  885/3125   train_loss = 0.962\n",
      "2022-02-11T23:17:40.073428: Epoch   3 Batch  905/3125   train_loss = 1.036\n",
      "2022-02-11T23:17:40.911041: Epoch   3 Batch  925/3125   train_loss = 0.929\n",
      "2022-02-11T23:17:41.708408: Epoch   3 Batch  945/3125   train_loss = 0.994\n",
      "2022-02-11T23:17:42.502124: Epoch   3 Batch  965/3125   train_loss = 0.766\n",
      "2022-02-11T23:17:43.251589: Epoch   3 Batch  985/3125   train_loss = 0.969\n",
      "2022-02-11T23:17:44.049505: Epoch   3 Batch 1005/3125   train_loss = 0.868\n",
      "2022-02-11T23:17:44.756883: Epoch   3 Batch 1025/3125   train_loss = 0.898\n",
      "2022-02-11T23:17:45.481372: Epoch   3 Batch 1045/3125   train_loss = 1.122\n",
      "2022-02-11T23:17:46.260618: Epoch   3 Batch 1065/3125   train_loss = 0.927\n",
      "2022-02-11T23:17:47.040042: Epoch   3 Batch 1085/3125   train_loss = 0.811\n",
      "2022-02-11T23:17:47.767534: Epoch   3 Batch 1105/3125   train_loss = 0.869\n",
      "2022-02-11T23:17:48.599079: Epoch   3 Batch 1125/3125   train_loss = 0.872\n",
      "2022-02-11T23:17:49.407471: Epoch   3 Batch 1145/3125   train_loss = 0.899\n",
      "2022-02-11T23:17:50.173402: Epoch   3 Batch 1165/3125   train_loss = 0.985\n",
      "2022-02-11T23:17:50.902064: Epoch   3 Batch 1185/3125   train_loss = 0.853\n",
      "2022-02-11T23:17:51.732665: Epoch   3 Batch 1205/3125   train_loss = 0.847\n",
      "2022-02-11T23:17:52.496159: Epoch   3 Batch 1225/3125   train_loss = 0.965\n",
      "2022-02-11T23:17:53.325617: Epoch   3 Batch 1245/3125   train_loss = 1.074\n",
      "2022-02-11T23:17:54.097365: Epoch   3 Batch 1265/3125   train_loss = 0.907\n",
      "2022-02-11T23:17:54.891662: Epoch   3 Batch 1285/3125   train_loss = 1.008\n",
      "2022-02-11T23:17:55.642950: Epoch   3 Batch 1305/3125   train_loss = 0.781\n",
      "2022-02-11T23:17:56.370112: Epoch   3 Batch 1325/3125   train_loss = 0.867\n",
      "2022-02-11T23:17:57.125721: Epoch   3 Batch 1345/3125   train_loss = 0.928\n",
      "2022-02-11T23:17:57.907479: Epoch   3 Batch 1365/3125   train_loss = 0.866\n",
      "2022-02-11T23:17:58.728811: Epoch   3 Batch 1385/3125   train_loss = 0.766\n",
      "2022-02-11T23:17:59.495526: Epoch   3 Batch 1405/3125   train_loss = 0.924\n",
      "2022-02-11T23:18:00.312695: Epoch   3 Batch 1425/3125   train_loss = 1.039\n",
      "2022-02-11T23:18:01.062852: Epoch   3 Batch 1445/3125   train_loss = 1.050\n",
      "2022-02-11T23:18:01.867773: Epoch   3 Batch 1465/3125   train_loss = 0.861\n",
      "2022-02-11T23:18:02.528713: Epoch   3 Batch 1485/3125   train_loss = 0.982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-11T23:18:03.287279: Epoch   3 Batch 1505/3125   train_loss = 0.741\n",
      "2022-02-11T23:18:04.064501: Epoch   3 Batch 1525/3125   train_loss = 0.778\n",
      "2022-02-11T23:18:04.877182: Epoch   3 Batch 1545/3125   train_loss = 0.849\n",
      "2022-02-11T23:18:05.689142: Epoch   3 Batch 1565/3125   train_loss = 0.931\n",
      "2022-02-11T23:18:06.479823: Epoch   3 Batch 1585/3125   train_loss = 0.871\n",
      "2022-02-11T23:18:07.348688: Epoch   3 Batch 1605/3125   train_loss = 0.923\n",
      "2022-02-11T23:18:08.205226: Epoch   3 Batch 1625/3125   train_loss = 0.968\n",
      "2022-02-11T23:18:08.981574: Epoch   3 Batch 1645/3125   train_loss = 1.025\n",
      "2022-02-11T23:18:09.815728: Epoch   3 Batch 1665/3125   train_loss = 0.973\n",
      "2022-02-11T23:18:10.695887: Epoch   3 Batch 1685/3125   train_loss = 1.012\n",
      "2022-02-11T23:18:11.542224: Epoch   3 Batch 1705/3125   train_loss = 0.928\n",
      "2022-02-11T23:18:12.363710: Epoch   3 Batch 1725/3125   train_loss = 0.854\n",
      "2022-02-11T23:18:13.244934: Epoch   3 Batch 1745/3125   train_loss = 0.821\n",
      "2022-02-11T23:18:14.045172: Epoch   3 Batch 1765/3125   train_loss = 0.862\n",
      "2022-02-11T23:18:14.722030: Epoch   3 Batch 1785/3125   train_loss = 1.017\n",
      "2022-02-11T23:18:15.396197: Epoch   3 Batch 1805/3125   train_loss = 0.969\n",
      "2022-02-11T23:18:16.265218: Epoch   3 Batch 1825/3125   train_loss = 0.991\n",
      "2022-02-11T23:18:17.143392: Epoch   3 Batch 1845/3125   train_loss = 0.928\n",
      "2022-02-11T23:18:18.005808: Epoch   3 Batch 1865/3125   train_loss = 0.761\n",
      "2022-02-11T23:18:18.854182: Epoch   3 Batch 1885/3125   train_loss = 0.939\n",
      "2022-02-11T23:18:19.730509: Epoch   3 Batch 1905/3125   train_loss = 0.800\n",
      "2022-02-11T23:18:20.498054: Epoch   3 Batch 1925/3125   train_loss = 0.842\n",
      "2022-02-11T23:18:21.337874: Epoch   3 Batch 1945/3125   train_loss = 0.903\n",
      "2022-02-11T23:18:21.981307: Epoch   3 Batch 1965/3125   train_loss = 0.860\n",
      "2022-02-11T23:18:22.610158: Epoch   3 Batch 1985/3125   train_loss = 0.887\n",
      "2022-02-11T23:18:23.255742: Epoch   3 Batch 2005/3125   train_loss = 0.880\n",
      "2022-02-11T23:18:24.044641: Epoch   3 Batch 2025/3125   train_loss = 0.985\n",
      "2022-02-11T23:18:24.852782: Epoch   3 Batch 2045/3125   train_loss = 0.773\n",
      "2022-02-11T23:18:25.628191: Epoch   3 Batch 2065/3125   train_loss = 0.758\n",
      "2022-02-11T23:18:26.338582: Epoch   3 Batch 2085/3125   train_loss = 0.996\n",
      "2022-02-11T23:18:27.022469: Epoch   3 Batch 2105/3125   train_loss = 0.846\n",
      "2022-02-11T23:18:27.839420: Epoch   3 Batch 2125/3125   train_loss = 0.992\n",
      "2022-02-11T23:18:28.647658: Epoch   3 Batch 2145/3125   train_loss = 0.994\n",
      "2022-02-11T23:18:29.496819: Epoch   3 Batch 2165/3125   train_loss = 0.865\n",
      "2022-02-11T23:18:30.387733: Epoch   3 Batch 2185/3125   train_loss = 0.991\n",
      "2022-02-11T23:18:31.254679: Epoch   3 Batch 2205/3125   train_loss = 0.947\n",
      "2022-02-11T23:18:32.135024: Epoch   3 Batch 2225/3125   train_loss = 0.856\n",
      "2022-02-11T23:18:32.883371: Epoch   3 Batch 2245/3125   train_loss = 0.801\n",
      "2022-02-11T23:18:33.695375: Epoch   3 Batch 2265/3125   train_loss = 0.866\n",
      "2022-02-11T23:18:34.599321: Epoch   3 Batch 2285/3125   train_loss = 1.054\n",
      "2022-02-11T23:18:35.454933: Epoch   3 Batch 2305/3125   train_loss = 0.795\n",
      "2022-02-11T23:18:36.285518: Epoch   3 Batch 2325/3125   train_loss = 0.834\n",
      "2022-02-11T23:18:37.140274: Epoch   3 Batch 2345/3125   train_loss = 0.894\n",
      "2022-02-11T23:18:37.965976: Epoch   3 Batch 2365/3125   train_loss = 0.750\n",
      "2022-02-11T23:18:38.730243: Epoch   3 Batch 2385/3125   train_loss = 0.949\n",
      "2022-02-11T23:18:39.569934: Epoch   3 Batch 2405/3125   train_loss = 0.953\n",
      "2022-02-11T23:18:40.432821: Epoch   3 Batch 2425/3125   train_loss = 0.834\n",
      "2022-02-11T23:18:41.338798: Epoch   3 Batch 2445/3125   train_loss = 0.966\n",
      "2022-02-11T23:18:42.152429: Epoch   3 Batch 2465/3125   train_loss = 0.749\n",
      "2022-02-11T23:18:43.016687: Epoch   3 Batch 2485/3125   train_loss = 0.859\n",
      "2022-02-11T23:18:43.855211: Epoch   3 Batch 2505/3125   train_loss = 0.849\n",
      "2022-02-11T23:18:44.709430: Epoch   3 Batch 2525/3125   train_loss = 0.893\n",
      "2022-02-11T23:18:45.537918: Epoch   3 Batch 2545/3125   train_loss = 1.046\n",
      "2022-02-11T23:18:46.364981: Epoch   3 Batch 2565/3125   train_loss = 0.865\n",
      "2022-02-11T23:18:47.231430: Epoch   3 Batch 2585/3125   train_loss = 0.847\n",
      "2022-02-11T23:18:47.962290: Epoch   3 Batch 2605/3125   train_loss = 0.813\n",
      "2022-02-11T23:18:48.772431: Epoch   3 Batch 2625/3125   train_loss = 0.984\n",
      "2022-02-11T23:18:49.585842: Epoch   3 Batch 2645/3125   train_loss = 0.895\n",
      "2022-02-11T23:18:50.446226: Epoch   3 Batch 2665/3125   train_loss = 0.999\n",
      "2022-02-11T23:18:51.118498: Epoch   3 Batch 2685/3125   train_loss = 0.911\n",
      "2022-02-11T23:18:51.961106: Epoch   3 Batch 2705/3125   train_loss = 0.817\n",
      "2022-02-11T23:18:52.763751: Epoch   3 Batch 2725/3125   train_loss = 0.941\n",
      "2022-02-11T23:18:53.539549: Epoch   3 Batch 2745/3125   train_loss = 0.936\n",
      "2022-02-11T23:18:54.263111: Epoch   3 Batch 2765/3125   train_loss = 0.809\n",
      "2022-02-11T23:18:55.066641: Epoch   3 Batch 2785/3125   train_loss = 1.003\n",
      "2022-02-11T23:18:55.889183: Epoch   3 Batch 2805/3125   train_loss = 0.818\n",
      "2022-02-11T23:18:56.658623: Epoch   3 Batch 2825/3125   train_loss = 0.907\n",
      "2022-02-11T23:18:57.450853: Epoch   3 Batch 2845/3125   train_loss = 0.922\n",
      "2022-02-11T23:18:58.374095: Epoch   3 Batch 2865/3125   train_loss = 0.859\n",
      "2022-02-11T23:18:59.183444: Epoch   3 Batch 2885/3125   train_loss = 0.936\n",
      "2022-02-11T23:18:59.915855: Epoch   3 Batch 2905/3125   train_loss = 0.991\n",
      "2022-02-11T23:19:00.559844: Epoch   3 Batch 2925/3125   train_loss = 0.901\n",
      "2022-02-11T23:19:01.384295: Epoch   3 Batch 2945/3125   train_loss = 0.974\n",
      "2022-02-11T23:19:02.128451: Epoch   3 Batch 2965/3125   train_loss = 0.973\n",
      "2022-02-11T23:19:02.854984: Epoch   3 Batch 2985/3125   train_loss = 0.851\n",
      "2022-02-11T23:19:03.541477: Epoch   3 Batch 3005/3125   train_loss = 0.865\n",
      "2022-02-11T23:19:04.307563: Epoch   3 Batch 3025/3125   train_loss = 0.973\n",
      "2022-02-11T23:19:05.050823: Epoch   3 Batch 3045/3125   train_loss = 0.938\n",
      "2022-02-11T23:19:05.793009: Epoch   3 Batch 3065/3125   train_loss = 0.828\n",
      "2022-02-11T23:19:06.612380: Epoch   3 Batch 3085/3125   train_loss = 0.861\n",
      "2022-02-11T23:19:07.455933: Epoch   3 Batch 3105/3125   train_loss = 0.882\n",
      "2022-02-11T23:19:08.373190: Epoch   3 Batch   17/781   test_loss = 0.899\n",
      "2022-02-11T23:19:08.588346: Epoch   3 Batch   37/781   test_loss = 0.885\n",
      "2022-02-11T23:19:08.855917: Epoch   3 Batch   57/781   test_loss = 0.972\n",
      "2022-02-11T23:19:09.115543: Epoch   3 Batch   77/781   test_loss = 0.869\n",
      "2022-02-11T23:19:09.374635: Epoch   3 Batch   97/781   test_loss = 0.742\n",
      "2022-02-11T23:19:09.571795: Epoch   3 Batch  117/781   test_loss = 0.987\n",
      "2022-02-11T23:19:09.789767: Epoch   3 Batch  137/781   test_loss = 0.944\n",
      "2022-02-11T23:19:10.007948: Epoch   3 Batch  157/781   test_loss = 0.906\n",
      "2022-02-11T23:19:10.226568: Epoch   3 Batch  177/781   test_loss = 0.856\n",
      "2022-02-11T23:19:10.480502: Epoch   3 Batch  197/781   test_loss = 0.908\n",
      "2022-02-11T23:19:10.735600: Epoch   3 Batch  217/781   test_loss = 0.763\n",
      "2022-02-11T23:19:11.002529: Epoch   3 Batch  237/781   test_loss = 0.759\n",
      "2022-02-11T23:19:11.201454: Epoch   3 Batch  257/781   test_loss = 0.997\n",
      "2022-02-11T23:19:11.432131: Epoch   3 Batch  277/781   test_loss = 0.915\n",
      "2022-02-11T23:19:11.730778: Epoch   3 Batch  297/781   test_loss = 0.975\n",
      "2022-02-11T23:19:11.963255: Epoch   3 Batch  317/781   test_loss = 1.044\n",
      "2022-02-11T23:19:12.166347: Epoch   3 Batch  337/781   test_loss = 0.913\n",
      "2022-02-11T23:19:12.369269: Epoch   3 Batch  357/781   test_loss = 0.904\n",
      "2022-02-11T23:19:12.569362: Epoch   3 Batch  377/781   test_loss = 0.926\n",
      "2022-02-11T23:19:12.831797: Epoch   3 Batch  397/781   test_loss = 0.969\n",
      "2022-02-11T23:19:13.056878: Epoch   3 Batch  417/781   test_loss = 0.832\n",
      "2022-02-11T23:19:13.259618: Epoch   3 Batch  437/781   test_loss = 0.795\n",
      "2022-02-11T23:19:13.489213: Epoch   3 Batch  457/781   test_loss = 0.717\n",
      "2022-02-11T23:19:13.724614: Epoch   3 Batch  477/781   test_loss = 0.903\n",
      "2022-02-11T23:19:13.976900: Epoch   3 Batch  497/781   test_loss = 0.813\n",
      "2022-02-11T23:19:14.227627: Epoch   3 Batch  517/781   test_loss = 0.824\n",
      "2022-02-11T23:19:14.450696: Epoch   3 Batch  537/781   test_loss = 0.875\n",
      "2022-02-11T23:19:14.688030: Epoch   3 Batch  557/781   test_loss = 1.050\n",
      "2022-02-11T23:19:15.000244: Epoch   3 Batch  577/781   test_loss = 0.922\n",
      "2022-02-11T23:19:15.234709: Epoch   3 Batch  597/781   test_loss = 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-11T23:19:15.489422: Epoch   3 Batch  617/781   test_loss = 0.871\n",
      "2022-02-11T23:19:15.725571: Epoch   3 Batch  637/781   test_loss = 0.820\n",
      "2022-02-11T23:19:15.987506: Epoch   3 Batch  657/781   test_loss = 1.011\n",
      "2022-02-11T23:19:16.209920: Epoch   3 Batch  677/781   test_loss = 0.907\n",
      "2022-02-11T23:19:16.422120: Epoch   3 Batch  697/781   test_loss = 0.954\n",
      "2022-02-11T23:19:16.633492: Epoch   3 Batch  717/781   test_loss = 0.838\n",
      "2022-02-11T23:19:16.904063: Epoch   3 Batch  737/781   test_loss = 0.741\n",
      "2022-02-11T23:19:17.157213: Epoch   3 Batch  757/781   test_loss = 1.061\n",
      "2022-02-11T23:19:17.405616: Epoch   3 Batch  777/781   test_loss = 0.913\n",
      "2022-02-11T23:19:17.938406: Epoch   4 Batch    0/3125   train_loss = 0.926\n",
      "2022-02-11T23:19:18.727993: Epoch   4 Batch   20/3125   train_loss = 0.878\n",
      "2022-02-11T23:19:19.636943: Epoch   4 Batch   40/3125   train_loss = 0.961\n",
      "2022-02-11T23:19:20.477302: Epoch   4 Batch   60/3125   train_loss = 0.804\n",
      "2022-02-11T23:19:21.217985: Epoch   4 Batch   80/3125   train_loss = 0.890\n",
      "2022-02-11T23:19:21.896913: Epoch   4 Batch  100/3125   train_loss = 0.959\n",
      "2022-02-11T23:19:22.680247: Epoch   4 Batch  120/3125   train_loss = 1.012\n",
      "2022-02-11T23:19:23.433575: Epoch   4 Batch  140/3125   train_loss = 0.951\n",
      "2022-02-11T23:19:24.167988: Epoch   4 Batch  160/3125   train_loss = 0.793\n",
      "2022-02-11T23:19:24.931886: Epoch   4 Batch  180/3125   train_loss = 0.910\n",
      "2022-02-11T23:19:25.600590: Epoch   4 Batch  200/3125   train_loss = 1.105\n",
      "2022-02-11T23:19:26.299399: Epoch   4 Batch  220/3125   train_loss = 0.889\n",
      "2022-02-11T23:19:26.982531: Epoch   4 Batch  240/3125   train_loss = 0.997\n",
      "2022-02-11T23:19:27.839093: Epoch   4 Batch  260/3125   train_loss = 0.934\n",
      "2022-02-11T23:19:28.668099: Epoch   4 Batch  280/3125   train_loss = 0.971\n",
      "2022-02-11T23:19:29.502158: Epoch   4 Batch  300/3125   train_loss = 1.063\n",
      "2022-02-11T23:19:30.296383: Epoch   4 Batch  320/3125   train_loss = 0.966\n",
      "2022-02-11T23:19:31.072889: Epoch   4 Batch  340/3125   train_loss = 0.733\n",
      "2022-02-11T23:19:31.890853: Epoch   4 Batch  360/3125   train_loss = 0.865\n",
      "2022-02-11T23:19:32.643514: Epoch   4 Batch  380/3125   train_loss = 0.882\n",
      "2022-02-11T23:19:33.376956: Epoch   4 Batch  400/3125   train_loss = 0.870\n",
      "2022-02-11T23:19:34.185187: Epoch   4 Batch  420/3125   train_loss = 0.787\n",
      "2022-02-11T23:19:34.967962: Epoch   4 Batch  440/3125   train_loss = 0.861\n",
      "2022-02-11T23:19:35.800091: Epoch   4 Batch  460/3125   train_loss = 0.866\n",
      "2022-02-11T23:19:36.615061: Epoch   4 Batch  480/3125   train_loss = 0.997\n",
      "2022-02-11T23:19:37.415214: Epoch   4 Batch  500/3125   train_loss = 0.653\n",
      "2022-02-11T23:19:38.230061: Epoch   4 Batch  520/3125   train_loss = 0.888\n",
      "2022-02-11T23:19:39.026337: Epoch   4 Batch  540/3125   train_loss = 0.792\n",
      "2022-02-11T23:19:39.859108: Epoch   4 Batch  560/3125   train_loss = 1.028\n",
      "2022-02-11T23:19:40.663165: Epoch   4 Batch  580/3125   train_loss = 0.983\n",
      "2022-02-11T23:19:41.442266: Epoch   4 Batch  600/3125   train_loss = 0.919\n",
      "2022-02-11T23:19:42.241844: Epoch   4 Batch  620/3125   train_loss = 0.872\n",
      "2022-02-11T23:19:43.027963: Epoch   4 Batch  640/3125   train_loss = 0.886\n",
      "2022-02-11T23:19:43.767205: Epoch   4 Batch  660/3125   train_loss = 0.893\n",
      "2022-02-11T23:19:44.481440: Epoch   4 Batch  680/3125   train_loss = 0.923\n",
      "2022-02-11T23:19:45.204878: Epoch   4 Batch  700/3125   train_loss = 0.940\n",
      "2022-02-11T23:19:45.972566: Epoch   4 Batch  720/3125   train_loss = 0.805\n",
      "2022-02-11T23:19:46.825244: Epoch   4 Batch  740/3125   train_loss = 0.897\n",
      "2022-02-11T23:19:47.678197: Epoch   4 Batch  760/3125   train_loss = 0.771\n",
      "2022-02-11T23:19:48.482195: Epoch   4 Batch  780/3125   train_loss = 0.916\n",
      "2022-02-11T23:19:49.250387: Epoch   4 Batch  800/3125   train_loss = 0.820\n",
      "2022-02-11T23:19:50.056483: Epoch   4 Batch  820/3125   train_loss = 0.851\n",
      "2022-02-11T23:19:50.941818: Epoch   4 Batch  840/3125   train_loss = 0.855\n",
      "2022-02-11T23:19:51.733751: Epoch   4 Batch  860/3125   train_loss = 0.822\n",
      "2022-02-11T23:19:52.510817: Epoch   4 Batch  880/3125   train_loss = 0.781\n",
      "2022-02-11T23:19:53.132412: Epoch   4 Batch  900/3125   train_loss = 0.853\n",
      "2022-02-11T23:19:53.742422: Epoch   4 Batch  920/3125   train_loss = 0.970\n",
      "2022-02-11T23:19:54.509491: Epoch   4 Batch  940/3125   train_loss = 0.861\n",
      "2022-02-11T23:19:55.223068: Epoch   4 Batch  960/3125   train_loss = 0.936\n",
      "2022-02-11T23:19:56.076258: Epoch   4 Batch  980/3125   train_loss = 0.970\n",
      "2022-02-11T23:19:56.821487: Epoch   4 Batch 1000/3125   train_loss = 0.958\n",
      "2022-02-11T23:19:57.446602: Epoch   4 Batch 1020/3125   train_loss = 0.921\n",
      "2022-02-11T23:19:58.244010: Epoch   4 Batch 1040/3125   train_loss = 0.783\n",
      "2022-02-11T23:19:59.043723: Epoch   4 Batch 1060/3125   train_loss = 0.915\n",
      "2022-02-11T23:19:59.830865: Epoch   4 Batch 1080/3125   train_loss = 0.883\n",
      "2022-02-11T23:20:00.612133: Epoch   4 Batch 1100/3125   train_loss = 0.842\n",
      "2022-02-11T23:20:01.429682: Epoch   4 Batch 1120/3125   train_loss = 0.847\n",
      "2022-02-11T23:20:02.240199: Epoch   4 Batch 1140/3125   train_loss = 0.956\n",
      "2022-02-11T23:20:03.058364: Epoch   4 Batch 1160/3125   train_loss = 0.792\n",
      "2022-02-11T23:20:03.863266: Epoch   4 Batch 1180/3125   train_loss = 0.815\n",
      "2022-02-11T23:20:04.631762: Epoch   4 Batch 1200/3125   train_loss = 1.032\n",
      "2022-02-11T23:20:05.398414: Epoch   4 Batch 1220/3125   train_loss = 0.969\n",
      "2022-02-11T23:20:06.178825: Epoch   4 Batch 1240/3125   train_loss = 0.750\n",
      "2022-02-11T23:20:06.966635: Epoch   4 Batch 1260/3125   train_loss = 0.870\n",
      "2022-02-11T23:20:07.800475: Epoch   4 Batch 1280/3125   train_loss = 0.890\n",
      "2022-02-11T23:20:08.594461: Epoch   4 Batch 1300/3125   train_loss = 0.831\n",
      "2022-02-11T23:20:09.367065: Epoch   4 Batch 1320/3125   train_loss = 0.920\n",
      "2022-02-11T23:20:10.032823: Epoch   4 Batch 1340/3125   train_loss = 0.756\n",
      "2022-02-11T23:20:10.719900: Epoch   4 Batch 1360/3125   train_loss = 0.854\n",
      "2022-02-11T23:20:11.448901: Epoch   4 Batch 1380/3125   train_loss = 0.802\n",
      "2022-02-11T23:20:12.185900: Epoch   4 Batch 1400/3125   train_loss = 0.913\n",
      "2022-02-11T23:20:12.837024: Epoch   4 Batch 1420/3125   train_loss = 0.941\n",
      "2022-02-11T23:20:13.489891: Epoch   4 Batch 1440/3125   train_loss = 0.777\n",
      "2022-02-11T23:20:14.192238: Epoch   4 Batch 1460/3125   train_loss = 0.845\n",
      "2022-02-11T23:20:15.052406: Epoch   4 Batch 1480/3125   train_loss = 0.872\n",
      "2022-02-11T23:20:15.829490: Epoch   4 Batch 1500/3125   train_loss = 0.892\n",
      "2022-02-11T23:20:16.480689: Epoch   4 Batch 1520/3125   train_loss = 0.829\n",
      "2022-02-11T23:20:17.199457: Epoch   4 Batch 1540/3125   train_loss = 0.939\n",
      "2022-02-11T23:20:17.910291: Epoch   4 Batch 1560/3125   train_loss = 0.832\n",
      "2022-02-11T23:20:18.663910: Epoch   4 Batch 1580/3125   train_loss = 0.958\n",
      "2022-02-11T23:20:19.340044: Epoch   4 Batch 1600/3125   train_loss = 0.867\n",
      "2022-02-11T23:20:20.100307: Epoch   4 Batch 1620/3125   train_loss = 0.814\n",
      "2022-02-11T23:20:20.855784: Epoch   4 Batch 1640/3125   train_loss = 0.901\n",
      "2022-02-11T23:20:21.662180: Epoch   4 Batch 1660/3125   train_loss = 0.991\n",
      "2022-02-11T23:20:22.465136: Epoch   4 Batch 1680/3125   train_loss = 0.905\n",
      "2022-02-11T23:20:23.193695: Epoch   4 Batch 1700/3125   train_loss = 0.741\n",
      "2022-02-11T23:20:23.908969: Epoch   4 Batch 1720/3125   train_loss = 0.895\n",
      "2022-02-11T23:20:24.698223: Epoch   4 Batch 1740/3125   train_loss = 0.958\n",
      "2022-02-11T23:20:25.495901: Epoch   4 Batch 1760/3125   train_loss = 0.906\n",
      "2022-02-11T23:20:26.290818: Epoch   4 Batch 1780/3125   train_loss = 0.903\n",
      "2022-02-11T23:20:27.053007: Epoch   4 Batch 1800/3125   train_loss = 0.835\n",
      "2022-02-11T23:20:27.870547: Epoch   4 Batch 1820/3125   train_loss = 0.832\n",
      "2022-02-11T23:20:28.665789: Epoch   4 Batch 1840/3125   train_loss = 0.945\n",
      "2022-02-11T23:20:29.458588: Epoch   4 Batch 1860/3125   train_loss = 1.005\n",
      "2022-02-11T23:20:30.226988: Epoch   4 Batch 1880/3125   train_loss = 0.857\n",
      "2022-02-11T23:20:31.013648: Epoch   4 Batch 1900/3125   train_loss = 0.748\n",
      "2022-02-11T23:20:31.639675: Epoch   4 Batch 1920/3125   train_loss = 0.857\n",
      "2022-02-11T23:20:32.419737: Epoch   4 Batch 1940/3125   train_loss = 0.783\n",
      "2022-02-11T23:20:33.177767: Epoch   4 Batch 1960/3125   train_loss = 0.779\n",
      "2022-02-11T23:20:33.938600: Epoch   4 Batch 1980/3125   train_loss = 0.840\n",
      "2022-02-11T23:20:34.749052: Epoch   4 Batch 2000/3125   train_loss = 1.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-11T23:20:35.517710: Epoch   4 Batch 2020/3125   train_loss = 0.965\n",
      "2022-02-11T23:20:36.266617: Epoch   4 Batch 2040/3125   train_loss = 0.783\n",
      "2022-02-11T23:20:37.026363: Epoch   4 Batch 2060/3125   train_loss = 0.839\n",
      "2022-02-11T23:20:37.749144: Epoch   4 Batch 2080/3125   train_loss = 1.027\n",
      "2022-02-11T23:20:38.499147: Epoch   4 Batch 2100/3125   train_loss = 0.785\n",
      "2022-02-11T23:20:39.286514: Epoch   4 Batch 2120/3125   train_loss = 0.805\n",
      "2022-02-11T23:20:40.091359: Epoch   4 Batch 2140/3125   train_loss = 0.867\n",
      "2022-02-11T23:20:40.905397: Epoch   4 Batch 2160/3125   train_loss = 0.840\n",
      "2022-02-11T23:20:41.669081: Epoch   4 Batch 2180/3125   train_loss = 0.927\n",
      "2022-02-11T23:20:42.476604: Epoch   4 Batch 2200/3125   train_loss = 0.790\n",
      "2022-02-11T23:20:43.136633: Epoch   4 Batch 2220/3125   train_loss = 0.857\n",
      "2022-02-11T23:20:43.835323: Epoch   4 Batch 2240/3125   train_loss = 0.806\n",
      "2022-02-11T23:20:44.549393: Epoch   4 Batch 2260/3125   train_loss = 0.867\n",
      "2022-02-11T23:20:45.218222: Epoch   4 Batch 2280/3125   train_loss = 0.882\n",
      "2022-02-11T23:20:45.921799: Epoch   4 Batch 2300/3125   train_loss = 0.869\n",
      "2022-02-11T23:20:46.669115: Epoch   4 Batch 2320/3125   train_loss = 0.966\n",
      "2022-02-11T23:20:47.551689: Epoch   4 Batch 2340/3125   train_loss = 0.867\n",
      "2022-02-11T23:20:48.355382: Epoch   4 Batch 2360/3125   train_loss = 0.911\n",
      "2022-02-11T23:20:49.178951: Epoch   4 Batch 2380/3125   train_loss = 0.838\n",
      "2022-02-11T23:20:49.972325: Epoch   4 Batch 2400/3125   train_loss = 0.994\n",
      "2022-02-11T23:20:50.795622: Epoch   4 Batch 2420/3125   train_loss = 0.819\n",
      "2022-02-11T23:20:51.478271: Epoch   4 Batch 2440/3125   train_loss = 0.839\n",
      "2022-02-11T23:20:52.291052: Epoch   4 Batch 2460/3125   train_loss = 0.851\n",
      "2022-02-11T23:20:53.131491: Epoch   4 Batch 2480/3125   train_loss = 0.970\n",
      "2022-02-11T23:20:53.928505: Epoch   4 Batch 2500/3125   train_loss = 0.845\n",
      "2022-02-11T23:20:54.790780: Epoch   4 Batch 2520/3125   train_loss = 0.931\n",
      "2022-02-11T23:20:55.620189: Epoch   4 Batch 2540/3125   train_loss = 0.826\n",
      "2022-02-11T23:20:56.472216: Epoch   4 Batch 2560/3125   train_loss = 0.661\n",
      "2022-02-11T23:20:57.203446: Epoch   4 Batch 2580/3125   train_loss = 0.868\n",
      "2022-02-11T23:20:58.049086: Epoch   4 Batch 2600/3125   train_loss = 0.899\n",
      "2022-02-11T23:20:58.743016: Epoch   4 Batch 2620/3125   train_loss = 0.815\n",
      "2022-02-11T23:20:59.541262: Epoch   4 Batch 2640/3125   train_loss = 0.817\n",
      "2022-02-11T23:21:00.308021: Epoch   4 Batch 2660/3125   train_loss = 1.052\n",
      "2022-02-11T23:21:01.058056: Epoch   4 Batch 2680/3125   train_loss = 0.805\n",
      "2022-02-11T23:21:01.809012: Epoch   4 Batch 2700/3125   train_loss = 0.877\n",
      "2022-02-11T23:21:02.655794: Epoch   4 Batch 2720/3125   train_loss = 0.767\n",
      "2022-02-11T23:21:03.301990: Epoch   4 Batch 2740/3125   train_loss = 0.896\n",
      "2022-02-11T23:21:04.071324: Epoch   4 Batch 2760/3125   train_loss = 0.785\n",
      "2022-02-11T23:21:04.848965: Epoch   4 Batch 2780/3125   train_loss = 0.849\n",
      "2022-02-11T23:21:05.486730: Epoch   4 Batch 2800/3125   train_loss = 1.017\n",
      "2022-02-11T23:21:06.142262: Epoch   4 Batch 2820/3125   train_loss = 1.050\n",
      "2022-02-11T23:21:06.845692: Epoch   4 Batch 2840/3125   train_loss = 0.841\n",
      "2022-02-11T23:21:07.547692: Epoch   4 Batch 2860/3125   train_loss = 0.809\n",
      "2022-02-11T23:21:08.244703: Epoch   4 Batch 2880/3125   train_loss = 0.880\n",
      "2022-02-11T23:21:08.920816: Epoch   4 Batch 2900/3125   train_loss = 0.826\n",
      "2022-02-11T23:21:09.593483: Epoch   4 Batch 2920/3125   train_loss = 0.881\n",
      "2022-02-11T23:21:10.257736: Epoch   4 Batch 2940/3125   train_loss = 0.911\n",
      "2022-02-11T23:21:11.010633: Epoch   4 Batch 2960/3125   train_loss = 0.914\n",
      "2022-02-11T23:21:11.711631: Epoch   4 Batch 2980/3125   train_loss = 0.834\n",
      "2022-02-11T23:21:12.414108: Epoch   4 Batch 3000/3125   train_loss = 0.948\n",
      "2022-02-11T23:21:13.152138: Epoch   4 Batch 3020/3125   train_loss = 0.996\n",
      "2022-02-11T23:21:13.928536: Epoch   4 Batch 3040/3125   train_loss = 0.895\n",
      "2022-02-11T23:21:14.768498: Epoch   4 Batch 3060/3125   train_loss = 0.783\n",
      "2022-02-11T23:21:15.508934: Epoch   4 Batch 3080/3125   train_loss = 1.007\n",
      "2022-02-11T23:21:16.306615: Epoch   4 Batch 3100/3125   train_loss = 1.083\n",
      "2022-02-11T23:21:17.067566: Epoch   4 Batch 3120/3125   train_loss = 0.843\n",
      "2022-02-11T23:21:17.490051: Epoch   4 Batch   16/781   test_loss = 0.803\n",
      "2022-02-11T23:21:17.729809: Epoch   4 Batch   36/781   test_loss = 0.927\n",
      "2022-02-11T23:21:17.928633: Epoch   4 Batch   56/781   test_loss = 0.926\n",
      "2022-02-11T23:21:18.121560: Epoch   4 Batch   76/781   test_loss = 0.956\n",
      "2022-02-11T23:21:18.352916: Epoch   4 Batch   96/781   test_loss = 0.996\n",
      "2022-02-11T23:21:18.581222: Epoch   4 Batch  116/781   test_loss = 0.833\n",
      "2022-02-11T23:21:18.807648: Epoch   4 Batch  136/781   test_loss = 0.817\n",
      "2022-02-11T23:21:18.998242: Epoch   4 Batch  156/781   test_loss = 0.893\n",
      "2022-02-11T23:21:19.223887: Epoch   4 Batch  176/781   test_loss = 0.905\n",
      "2022-02-11T23:21:19.475085: Epoch   4 Batch  196/781   test_loss = 0.793\n",
      "2022-02-11T23:21:19.715113: Epoch   4 Batch  216/781   test_loss = 0.937\n",
      "2022-02-11T23:21:19.913817: Epoch   4 Batch  236/781   test_loss = 0.808\n",
      "2022-02-11T23:21:20.153095: Epoch   4 Batch  256/781   test_loss = 0.810\n",
      "2022-02-11T23:21:20.405003: Epoch   4 Batch  276/781   test_loss = 1.078\n",
      "2022-02-11T23:21:20.641922: Epoch   4 Batch  296/781   test_loss = 0.838\n",
      "2022-02-11T23:21:20.828543: Epoch   4 Batch  316/781   test_loss = 0.844\n",
      "2022-02-11T23:21:21.073069: Epoch   4 Batch  336/781   test_loss = 0.790\n",
      "2022-02-11T23:21:21.276462: Epoch   4 Batch  356/781   test_loss = 0.886\n",
      "2022-02-11T23:21:21.513719: Epoch   4 Batch  376/781   test_loss = 0.888\n",
      "2022-02-11T23:21:21.749769: Epoch   4 Batch  396/781   test_loss = 0.837\n",
      "2022-02-11T23:21:22.008559: Epoch   4 Batch  416/781   test_loss = 0.958\n",
      "2022-02-11T23:21:22.245353: Epoch   4 Batch  436/781   test_loss = 0.907\n",
      "2022-02-11T23:21:22.504075: Epoch   4 Batch  456/781   test_loss = 0.733\n",
      "2022-02-11T23:21:22.722413: Epoch   4 Batch  476/781   test_loss = 0.907\n",
      "2022-02-11T23:21:22.904056: Epoch   4 Batch  496/781   test_loss = 0.940\n",
      "2022-02-11T23:21:23.098278: Epoch   4 Batch  516/781   test_loss = 0.779\n",
      "2022-02-11T23:21:23.323171: Epoch   4 Batch  536/781   test_loss = 0.946\n",
      "2022-02-11T23:21:23.585903: Epoch   4 Batch  556/781   test_loss = 0.778\n",
      "2022-02-11T23:21:23.808743: Epoch   4 Batch  576/781   test_loss = 0.964\n",
      "2022-02-11T23:21:24.008230: Epoch   4 Batch  596/781   test_loss = 1.007\n",
      "2022-02-11T23:21:24.226429: Epoch   4 Batch  616/781   test_loss = 0.958\n",
      "2022-02-11T23:21:24.444833: Epoch   4 Batch  636/781   test_loss = 0.884\n",
      "2022-02-11T23:21:24.672044: Epoch   4 Batch  656/781   test_loss = 0.871\n",
      "2022-02-11T23:21:24.909145: Epoch   4 Batch  676/781   test_loss = 1.081\n",
      "2022-02-11T23:21:25.194160: Epoch   4 Batch  696/781   test_loss = 0.859\n",
      "2022-02-11T23:21:25.404819: Epoch   4 Batch  716/781   test_loss = 0.900\n",
      "2022-02-11T23:21:25.683587: Epoch   4 Batch  736/781   test_loss = 1.058\n",
      "2022-02-11T23:21:25.926766: Epoch   4 Batch  756/781   test_loss = 0.797\n",
      "2022-02-11T23:21:26.204809: Epoch   4 Batch  776/781   test_loss = 0.746\n",
      "Model Trained and Saved, test loss is {}\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # 摘要的输出目录\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    out_dir == \"C:/Desktop/movie/movie_recommend_cnn/\"\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # 损失的摘要\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # 训练摘要\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # 推理摘要Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "             # 取得批量数据\n",
    "            x, y = next(train_batches)\n",
    "           # 准备传入给计算图的数据\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "            #开始训练\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            #保存损失\n",
    "            losses['train'].append(train_loss)\n",
    "            #保存摘要给tensorboard\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ddc8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2392901a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f5298e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 \n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d022911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d9d5b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bcc4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c02398f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8abdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d64a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "      \n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111ffc88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1d77f57d4658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrecommend_your_favorite_movie\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-6445361a0a11>\u001b[0m in \u001b[0;36mrecommend_your_favorite_movie\u001b[1;34m(user_id_val, top_k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrecommend_your_favorite_movie\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_id_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mloaded_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloaded_graph\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# Load saved model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "recommend_your_favorite_movie(23, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25108764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad201ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[23 'Assassins (1995)' 'Thriller']\n",
      "以下是给您的推荐：\n"
     ]
    }
   ],
   "source": [
    "recommend_same_type_movie(23, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93032dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a2fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
